{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q-MekX6dofD1"
   },
   "source": [
    "## Distilling the Knowledge in a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e97PMChjpFJX"
   },
   "source": [
    "Paper Url: https://arxiv.org/pdf/1503.02531.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VtvQHIrd1dgW"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V4w1Ad9ipOWN"
   },
   "source": [
    "### 1 Introduction\n",
    "#### 1.1 Motivation\n",
    "This tutorial will introduce how distilling the knowledge from a cumbersome and comprehensive neural network works. Specifically, it will show how distillation using soft targets can transfer a great deal of knowledge to the distilled model and that the distilled model performs better than the compressed model simply using the hard labels.\n",
    "\n",
    "In the ideal world, one way of improving the performance of machine learning models is to train\n",
    "a model with a large amount of data with appropriate regularizations, so that the model can generalize well on unseen data. However, this big model is computationally expensive and cumbersome to use. On the other hand, if we choose to use smaller models, they usually cannot capture many complex features that the more complex model can easily capture. \n",
    "\n",
    "The motivation for distilling the knowledge approach is that we would like to have a smaller model that learns the knowledge from the cumbersome model, demonstrates improved performances compared to typical smaller models, and is more suitable for deployment. In other words, we want to build a model that learns most of the generalizations from the cumbersome model and still is lighter. \n",
    "\n",
    "The big, cumbersome model is often called a \"teacher\" model and the smaller model that distills knoweldge from the teacher model is often called a \"student\" model. \n",
    "\n",
    "A \"teacher\" model assigns the biggest probability to the corect answer and relatively low probabilities to incorrect answers. The relative probabilities of incorrect answers show how the model tends to generalize. For example, an image of a Dog may have a very small chance of being mistaken for a Cat, but it is more likely than being mistaken for a Truck. \n",
    "\n",
    "A \"student\" model is trained to mimic a pre-trained, large, and cumbersome model. The following image is a simple explanation of how \"teacher\" transfers knowledge to \"student\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "colab_type": "code",
    "id": "htuVumbr18pU",
    "outputId": "1dce32dc-5796-4225-9d54-7f5cdeea7c0a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAGmCAYAAACDVxRJAAAAAXNSR0IArs4c6QAAAARnQU1BAACx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\n5tOW+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image('distill.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ib43peMk17_v"
   },
   "source": [
    "\n",
    "In distillation, instead of copying the weights or simplifying them, the objective is to train a smaller model on a transfer set and use a “soft target” distribution for each case in the transfer set. For this transfer stage, we use the same training set or a separate \"transfer\" set used for training the \"teacher\" model. The \"student\" model learns from the knowledge produced after training the \"teacher\" model with a high temperature in its softmax.\n",
    "\n",
    "#### 1.2 Insight\n",
    "In general, knowledge distillation is a simple and effective approach to remedying the insufficient supervised information (labels) in classification problem. In traditional classification problem, the target of model is to map all potential features to one discrete point in output space, such as one-hot encoding. Thus, with cross entropy, the information this point is able to provide would only be $\\log(\\text{# of classes})$. Nevertheless, with techniques of knowledge distillation, we can obtain a continuous distribution of labels from every sample, which offers more information than one-hot encoding, and also keeps the variance in classes and distance among classes.\n",
    "\n",
    "Therefore, we can regard knowledge distillation as an elegant technique for data augmentation, which would be very helpful for generalization. We know some other generalization methods in deep learning, such as dropout, L2 regularization, pre-train, etc. The intuition of knowledge distillation and pre-train are sort of similar, which want to incorporate some prior knowledge into neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KBOdkFDCTmPL"
   },
   "source": [
    "### 2 How Distillation Works: Using Softmax and Temperature\n",
    "Neural networks typically produce class probabilitties by using a \"softmax\" output layer that converts the logit (the inputs to the final softmax), $z_i$, computed for each class into a probability, $q_i$, by comparing $z_i$ with the other logits. Softmax function is commonly used for an output layer of neural networks, since it assigns probabilities for each label and the all probabilities add up to 1. It basically outputs a probability distribution of different labels. For the MNIST dataset, similarly, a softmax layer trained on the hand-written digits assign a separate probability for each of the ten digits and the probabilities add up to 1.   \n",
    "\n",
    "\n",
    "$$\n",
    "q_i = \\frac{exp(z_i)}{\\sum_j exp(z_i)}\n",
    "$$\n",
    "\n",
    "If distillation is incorporated, \n",
    "\n",
    "$$\n",
    "q_i = \\frac{exp(z_i/T)}{\\sum_j exp(z_j/T)}\n",
    "$$\n",
    "\n",
    "where T is a temperature, normally set to 1. If the value of T is big, all actions have nearly the same probability. Using a higher T produces a softer probability distribution over classes. On the other hand, if value of T is small, the more expected rewards affect the probability. For a lower T, the probability of the action with the highest expected reward tends to become 1.\n",
    "\n",
    "In distillation, we raise the temperature of the final softmax layer until the cumbersome model produces a suitably soft set of targets, where the probability is well-distributed across different labels. If we do not raise the temperature, the cumbersome model (\"teacher\") is likely to assign probability close to 1 to a single label and 0 to all the other labels. In that case, a \"student\" model will obtain information only about the one dominating label and not be able to obtain information from all the other labels.    \n",
    "\n",
    "A soft target approach is useful for a \"student\" model to effectively pick up the knowledge being transferred from a \"teacher\" model, since it can learn from each probability assigned to each label. For example, when the soft targets have high entropy, they tend to provide much more information than hard targets where classes are one-hot encoded (1 for the correct label and 0 for the other labels). \n",
    "\n",
    "In this tutorial, we train the distilled model both with soft targets and hard targets (correct labels). Specifically, we use a weighted average of two different objective functions based on soft targets and hard targets, respectively, which can be regarded as a multi-task learning problem. \n",
    "\n",
    "The first objective function $\\mathcal{L}_{soft}$ is the cross entropy with the soft targets and this cross entropy is computed using the same high temperature in the softmax of the distilled model as was used for generating the soft targets from the cumbersome model.\n",
    "\n",
    "$$\\mathcal{L}_{soft}=-\\sum_{i=1}^{K}p_i\\log q_i.$$\n",
    "\n",
    "where $p_i$ is the soft target from teacher model and $q_i$ is the softmax output from the student model. \n",
    "\n",
    "The second objective function $\\mathcal{L}_{hard}$ is the traditional cross entropy with the correct labels. This is computed using exactly the same logits in softmax of the distilled model but at a temperature of 1.\n",
    "\n",
    "Thus, we can get the final objective function is\n",
    "\n",
    "$$\\mathcal{L}=\\mathcal{L}_{hard} + \\lambda\\mathcal{L}_{soft}.$$\n",
    "\n",
    "where $\\lambda$ is the weight we assign for soft targets. Since the magnitudes of the gradients produced by the soft targets scale as $1/T^2$ it is important to multiply them by $T^2$ when using both hard and soft targets. In general, we should assign a larget weight to $\\mathcal{L}_{soft}$.\n",
    "\n",
    "After the student model has been trained (distilled as well), it uses a temperature of 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pebwP3WxFEhz"
   },
   "source": [
    "### 3 Data: MNIST and CIFAR-10\n",
    "\n",
    "We used two datasets for this tutorial: MNIST and CIFAR-10, since they are well-known datasets being widely used for training neural networks and easily accessible within Keras and Tensorflow platform. \n",
    "\n",
    "MNIST (Modified National Institute of Standards and Technology database) is a large database of 10 handwritten digits from 0 to 9. Being widely used in the paper, MNIST dataset contains 60,000 training images used for training the model and 10,000 testing images used for measuring the performance of the model on unseen test sets. Each image is 28 X 28 pixels, which can be interpreted as a big array of numbers. In this tutorial, we flattened this array into a vector of 28X28 = 784 numbers.\n",
    "\n",
    "CIFAR-10 dataset (Canadian Institute For Advanced Research) consists of a collection of images - airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck - in 10 classes, with 6000 images per class. It contains 50,000 training images and 10,000 test images. Each image is 32X32 color images, which can also be interpreted as a big array of numbers.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W-IjHIgfcETm"
   },
   "source": [
    "### 4 Implementations of Knowledge Distillation on MNIST\n",
    "#### 4.1 Fully-Connected Neural Networks\n",
    "First, in order to test distilliation on MNIST dataset, we utilized Tensorflow to implement the simple fully-connected neural networks in Section 3 in the original paper. The teacher network has two hidden layers with 1200 rectified linear hidden units at each layer, 50% dropout rate and L2 norm constraints with $l=15$ (presented in [G. E. Hinton, et al](https://arxiv.org/pdf/1207.0580.pdf)). The student network has two hidden layers with 800 rectified linear hidden units in each layer and no regularzation applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7yamjPcNh5Ch",
    "outputId": "a7b2d1e2-272b-4469-ca7d-0ceac0c9b59e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow.contrib.slim as slim\n",
    "import keras\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "\n",
    "def FCNetwork(input, dropout_rate=0.5, num_units=1200, scope='teacher',reuse = False):\n",
    "    with tf.variable_scope(scope,reuse = reuse) as sc :\n",
    "        with slim.arg_scope([slim.fully_connected],biases_initializer=tf.constant_initializer(0.0),activation_fn=tf.nn.relu): \n",
    "            net = slim.fully_connected(input, num_units, scope='fc1')\n",
    "            if scope == 'teacher':\n",
    "                net = tf.nn.dropout(net, dropout_rate)\n",
    "            net = slim.fully_connected(net, num_units, scope='fc2')\n",
    "            if scope == 'teacher':\n",
    "                net = tf.nn.dropout(net, dropout_rate)\n",
    "            net = slim.fully_connected(net, 10, activation_fn=None,scope='fc3')\n",
    "            return net\n",
    "\n",
    "def loss(prediction, output):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=output, logits=prediction)) # cross entropy                                           \n",
    "    correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(prediction),1), tf.argmax(output,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    return cross_entropy, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJNOc3pXoUM2"
   },
   "source": [
    "#### 4.2 Training, Test, and Distillation on MNIST\n",
    "We adopted gradient descent to optimize the model and set learning rate to 0.3. Also, we found out that the distilled student network needs more epochs to converge, so we ran 30k iterations for teacher network and 100k iterations for both distilled and undistilled student network. The batch size is set to 128 for both teacher and student networks. While traning both the cumbersome model and the smaller model, the tempature is set to 10 in order to produce a softer probability distribution over classes after experimenting on different parameters. Also, we assigned considerably low weight for hard target function (see details in code).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvMexLSUohKI"
   },
   "outputs": [],
   "source": [
    "def runDistillationMNIST(verbose=True, print_every=1000, teacher_iters=30000, student_iters=100000, \\\n",
    "                         batch_size=128, lr=0.4, teacher_num_units=1200, student_num_units=800, \\\n",
    "                         temperature=10, lam=10, keep_prob=0.5, l=15):\n",
    "    with tf.Graph().as_default():\n",
    "        # load data\n",
    "        mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)  \n",
    "\n",
    "        # placeholder\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "        dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "        # network output and divided by t\n",
    "        y_teacher = FCNetwork(x, dropout_rate, teacher_num_units, scope='teacher')\n",
    "        y_student = FCNetwork(x, num_units=student_num_units, scope='student')\n",
    "        y_teacher_t = y_teacher/temperature\n",
    "        y_student_t = y_student/temperature\n",
    "\n",
    "        # loss and acc\n",
    "        cross_entropy_teacher, accuracy_teacher = loss(y_teacher, y_)\n",
    "        student_loss1, _ = loss(y_student_t, tf.nn.softmax(y_teacher_t))\n",
    "        student_loss2, accuracy_student = loss(y_student, y_)\n",
    "        cross_entropy_student = (temperature**2 * lam * student_loss1 + student_loss2) / (temperature**2 * lam + 1)\n",
    "\n",
    "        # get vars and gradients\n",
    "        model_vars = tf.trainable_variables()\n",
    "        var_teacher = [var for var in model_vars if 'teacher' in var.name]\n",
    "        var_student = [var for var in model_vars if 'student' in var.name]\n",
    "        grad_teacher = tf.gradients(cross_entropy_teacher,var_teacher)\n",
    "        \n",
    "        # add coonstraints on teacher nn\n",
    "        grad_teacher = [tf.clip_by_norm(g, clip_norm=l, axes=0) for g in grad_teacher] \n",
    "        grad_student = tf.gradients(cross_entropy_student,var_student)\n",
    "\n",
    "        # use gradient descent\n",
    "        trainer_teacher = tf.train.GradientDescentOptimizer(lr)\n",
    "        trainer_student = tf.train.GradientDescentOptimizer(lr)\n",
    "        step_teacher = trainer_teacher.apply_gradients(zip(grad_teacher,var_teacher))\n",
    "        step_student = trainer_student.apply_gradients(zip(grad_student,var_student))\n",
    "\n",
    "        # sess\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # train and test teacher network\n",
    "        for i in range(teacher_iters):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            if i% print_every == 0:\n",
    "                train_acc_teacher = accuracy_teacher.eval(feed_dict={x:batch[0], y_: batch[1],dropout_rate: 1.0})\n",
    "                print(\"step %d, training accuracy %f\"%(i, train_acc_teacher))\n",
    "            step_teacher.run(feed_dict={x: batch[0], y_: batch[1], dropout_rate: keep_prob})\n",
    "\n",
    "        test_acc_teacher = sess.run(accuracy_teacher, feed_dict={x:mnist.test.images, y_: mnist.test.labels, dropout_rate: 1.0})\n",
    "        print(\"test accuracy of the teacher model is %f\"%(test_acc_teacher))\n",
    "\n",
    "        # train and test student network\n",
    "        for i in range(student_iters):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            if i% print_every == 0:\n",
    "                train_acc_student = accuracy_student.eval(feed_dict={x:batch[0], y_: batch[1],dropout_rate: 1.0})\n",
    "                print(\"step %d, training accuracy %f\"%(i, train_acc_student))\n",
    "            step_student.run(feed_dict={x: batch[0], y_: batch[1],dropout_rate: 1.0}) \n",
    "\n",
    "        test_acc_student = sess.run(accuracy_student, feed_dict={x:mnist.test.images, y_: mnist.test.labels, dropout_rate: 1.0})\n",
    "        print(\"test accuracy of the student model is %f\"%(test_acc_student))\n",
    "        \n",
    "        return test_acc_teacher, test_acc_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2383
    },
    "colab_type": "code",
    "id": "LD-kgQN4q7qp",
    "outputId": "b01993c7-f1ba-4ea4-8b05-9352fdcccf7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.054688\n",
      "step 1000, training accuracy 0.984375\n",
      "step 2000, training accuracy 0.984375\n",
      "step 3000, training accuracy 1.000000\n",
      "step 4000, training accuracy 0.992188\n",
      "step 5000, training accuracy 0.992188\n",
      "step 6000, training accuracy 0.992188\n",
      "step 7000, training accuracy 0.992188\n",
      "step 8000, training accuracy 0.992188\n",
      "step 9000, training accuracy 1.000000\n",
      "step 10000, training accuracy 1.000000\n",
      "step 11000, training accuracy 1.000000\n",
      "step 12000, training accuracy 0.992188\n",
      "step 13000, training accuracy 1.000000\n",
      "step 14000, training accuracy 1.000000\n",
      "step 15000, training accuracy 1.000000\n",
      "step 16000, training accuracy 1.000000\n",
      "step 17000, training accuracy 1.000000\n",
      "step 18000, training accuracy 1.000000\n",
      "step 19000, training accuracy 1.000000\n",
      "step 20000, training accuracy 1.000000\n",
      "step 21000, training accuracy 1.000000\n",
      "step 22000, training accuracy 0.992188\n",
      "step 23000, training accuracy 1.000000\n",
      "step 24000, training accuracy 1.000000\n",
      "step 25000, training accuracy 1.000000\n",
      "step 26000, training accuracy 1.000000\n",
      "step 27000, training accuracy 1.000000\n",
      "step 28000, training accuracy 1.000000\n",
      "step 29000, training accuracy 1.000000\n",
      "test accuracy of the teacher model is 0.987600\n",
      "step 0, training accuracy 0.140625\n",
      "step 1000, training accuracy 0.945312\n",
      "step 2000, training accuracy 0.960938\n",
      "step 3000, training accuracy 0.953125\n",
      "step 4000, training accuracy 0.945312\n",
      "step 5000, training accuracy 0.968750\n",
      "step 6000, training accuracy 0.976562\n",
      "step 7000, training accuracy 0.992188\n",
      "step 8000, training accuracy 0.992188\n",
      "step 9000, training accuracy 0.992188\n",
      "step 10000, training accuracy 0.984375\n",
      "step 11000, training accuracy 0.992188\n",
      "step 12000, training accuracy 0.992188\n",
      "step 13000, training accuracy 0.984375\n",
      "step 14000, training accuracy 0.984375\n",
      "step 15000, training accuracy 0.992188\n",
      "step 16000, training accuracy 1.000000\n",
      "step 17000, training accuracy 0.976562\n",
      "step 18000, training accuracy 0.992188\n",
      "step 19000, training accuracy 1.000000\n",
      "step 20000, training accuracy 1.000000\n",
      "step 21000, training accuracy 0.992188\n",
      "step 22000, training accuracy 1.000000\n",
      "step 23000, training accuracy 1.000000\n",
      "step 24000, training accuracy 0.992188\n",
      "step 25000, training accuracy 0.984375\n",
      "step 26000, training accuracy 1.000000\n",
      "step 27000, training accuracy 1.000000\n",
      "step 28000, training accuracy 1.000000\n",
      "step 29000, training accuracy 1.000000\n",
      "step 30000, training accuracy 0.992188\n",
      "step 31000, training accuracy 0.992188\n",
      "step 32000, training accuracy 1.000000\n",
      "step 33000, training accuracy 1.000000\n",
      "step 34000, training accuracy 1.000000\n",
      "step 35000, training accuracy 0.992188\n",
      "step 36000, training accuracy 1.000000\n",
      "step 37000, training accuracy 1.000000\n",
      "step 38000, training accuracy 0.992188\n",
      "step 39000, training accuracy 1.000000\n",
      "step 40000, training accuracy 1.000000\n",
      "step 41000, training accuracy 1.000000\n",
      "step 42000, training accuracy 1.000000\n",
      "step 43000, training accuracy 1.000000\n",
      "step 44000, training accuracy 0.992188\n",
      "step 45000, training accuracy 1.000000\n",
      "step 46000, training accuracy 1.000000\n",
      "step 47000, training accuracy 0.992188\n",
      "step 48000, training accuracy 0.992188\n",
      "step 49000, training accuracy 1.000000\n",
      "step 50000, training accuracy 1.000000\n",
      "step 51000, training accuracy 1.000000\n",
      "step 52000, training accuracy 1.000000\n",
      "step 53000, training accuracy 1.000000\n",
      "step 54000, training accuracy 1.000000\n",
      "step 55000, training accuracy 0.992188\n",
      "step 56000, training accuracy 1.000000\n",
      "step 57000, training accuracy 1.000000\n",
      "step 58000, training accuracy 1.000000\n",
      "step 59000, training accuracy 1.000000\n",
      "step 60000, training accuracy 1.000000\n",
      "step 61000, training accuracy 1.000000\n",
      "step 62000, training accuracy 0.992188\n",
      "step 63000, training accuracy 1.000000\n",
      "step 64000, training accuracy 1.000000\n",
      "step 65000, training accuracy 1.000000\n",
      "step 66000, training accuracy 1.000000\n",
      "step 67000, training accuracy 1.000000\n",
      "step 68000, training accuracy 1.000000\n",
      "step 69000, training accuracy 1.000000\n",
      "step 70000, training accuracy 1.000000\n",
      "step 71000, training accuracy 1.000000\n",
      "step 72000, training accuracy 1.000000\n",
      "step 73000, training accuracy 1.000000\n",
      "step 74000, training accuracy 1.000000\n",
      "step 75000, training accuracy 1.000000\n",
      "step 76000, training accuracy 1.000000\n",
      "step 77000, training accuracy 1.000000\n",
      "step 78000, training accuracy 1.000000\n",
      "step 79000, training accuracy 1.000000\n",
      "step 80000, training accuracy 0.992188\n",
      "step 81000, training accuracy 0.992188\n",
      "step 82000, training accuracy 0.992188\n",
      "step 83000, training accuracy 1.000000\n",
      "step 84000, training accuracy 1.000000\n",
      "step 85000, training accuracy 1.000000\n",
      "step 86000, training accuracy 1.000000\n",
      "step 87000, training accuracy 1.000000\n",
      "step 88000, training accuracy 1.000000\n",
      "step 89000, training accuracy 1.000000\n",
      "step 90000, training accuracy 1.000000\n",
      "step 91000, training accuracy 1.000000\n",
      "step 92000, training accuracy 1.000000\n",
      "step 93000, training accuracy 1.000000\n",
      "step 94000, training accuracy 1.000000\n",
      "step 95000, training accuracy 1.000000\n",
      "step 96000, training accuracy 1.000000\n",
      "step 97000, training accuracy 0.992188\n",
      "step 98000, training accuracy 1.000000\n",
      "step 99000, training accuracy 1.000000\n",
      "test accuracy of the student model is 0.985900\n"
     ]
    }
   ],
   "source": [
    "test_acc_teacher, test_acc_student = runDistillationMNIST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "omPhx38trEKt"
   },
   "outputs": [],
   "source": [
    "def runMNIST(verbose=True, print_every=1000, iters=100000, scope='teacher', num_hidden_units=1200, keep_prob=0.5, batch_size=128, lr=0.4):\n",
    "    with tf.Graph().as_default():\n",
    "        # load data\n",
    "        mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)  \n",
    "\n",
    "        # placeholder\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "        # network output and divided by t       \n",
    "        y = FCNetwork(x, keep_prob, num_hidden_units, scope=scope)\n",
    " \n",
    "        # loss and acc\n",
    "        cross_entropy, accuracy = loss(y, y_)\n",
    "\n",
    "        # use gradient descent to optimize\n",
    "        train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "        # sess\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # train and test student network\n",
    "        for i in range(iters):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            if i% print_every == 0:\n",
    "                train_acc = accuracy.eval(feed_dict={x:batch[0], y_: batch[1]})\n",
    "                print(\"step %d, training accuracy %f\"%(i, train_acc))\n",
    "            train_step.run(feed_dict={x: batch[0], y_: batch[1]}) \n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y_: mnist.test.labels})\n",
    "        print(\"test accuracy of the student model is %f\"%(test_acc))\n",
    "        \n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1856
    },
    "colab_type": "code",
    "id": "Ypl5X7IsyRZg",
    "outputId": "2f94f4b6-ea9d-44d3-e85b-67e50bcf940a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.093750\n",
      "step 1000, training accuracy 0.984375\n",
      "step 2000, training accuracy 0.992188\n",
      "step 3000, training accuracy 0.992188\n",
      "step 4000, training accuracy 0.992188\n",
      "step 5000, training accuracy 1.000000\n",
      "step 6000, training accuracy 1.000000\n",
      "step 7000, training accuracy 1.000000\n",
      "step 8000, training accuracy 1.000000\n",
      "step 9000, training accuracy 1.000000\n",
      "step 10000, training accuracy 1.000000\n",
      "step 11000, training accuracy 1.000000\n",
      "step 12000, training accuracy 1.000000\n",
      "step 13000, training accuracy 1.000000\n",
      "step 14000, training accuracy 1.000000\n",
      "step 15000, training accuracy 1.000000\n",
      "step 16000, training accuracy 1.000000\n",
      "step 17000, training accuracy 1.000000\n",
      "step 18000, training accuracy 1.000000\n",
      "step 19000, training accuracy 1.000000\n",
      "step 20000, training accuracy 1.000000\n",
      "step 21000, training accuracy 1.000000\n",
      "step 22000, training accuracy 1.000000\n",
      "step 23000, training accuracy 1.000000\n",
      "step 24000, training accuracy 1.000000\n",
      "step 25000, training accuracy 1.000000\n",
      "step 26000, training accuracy 1.000000\n",
      "step 27000, training accuracy 1.000000\n",
      "step 28000, training accuracy 1.000000\n",
      "step 29000, training accuracy 1.000000\n",
      "step 30000, training accuracy 1.000000\n",
      "step 31000, training accuracy 1.000000\n",
      "step 32000, training accuracy 1.000000\n",
      "step 33000, training accuracy 1.000000\n",
      "step 34000, training accuracy 1.000000\n",
      "step 35000, training accuracy 1.000000\n",
      "step 36000, training accuracy 1.000000\n",
      "step 37000, training accuracy 1.000000\n",
      "step 38000, training accuracy 1.000000\n",
      "step 39000, training accuracy 1.000000\n",
      "step 40000, training accuracy 1.000000\n",
      "step 41000, training accuracy 1.000000\n",
      "step 42000, training accuracy 1.000000\n",
      "step 43000, training accuracy 1.000000\n",
      "step 44000, training accuracy 1.000000\n",
      "step 45000, training accuracy 1.000000\n",
      "step 46000, training accuracy 1.000000\n",
      "step 47000, training accuracy 1.000000\n",
      "step 48000, training accuracy 1.000000\n",
      "step 49000, training accuracy 1.000000\n",
      "step 50000, training accuracy 1.000000\n",
      "step 51000, training accuracy 1.000000\n",
      "step 52000, training accuracy 1.000000\n",
      "step 53000, training accuracy 1.000000\n",
      "step 54000, training accuracy 1.000000\n",
      "step 55000, training accuracy 1.000000\n",
      "step 56000, training accuracy 1.000000\n",
      "step 57000, training accuracy 1.000000\n",
      "step 58000, training accuracy 1.000000\n",
      "step 59000, training accuracy 1.000000\n",
      "step 60000, training accuracy 1.000000\n",
      "step 61000, training accuracy 1.000000\n",
      "step 62000, training accuracy 1.000000\n",
      "step 63000, training accuracy 1.000000\n",
      "step 64000, training accuracy 1.000000\n",
      "step 65000, training accuracy 1.000000\n",
      "step 66000, training accuracy 1.000000\n",
      "step 67000, training accuracy 1.000000\n",
      "step 68000, training accuracy 1.000000\n",
      "step 69000, training accuracy 1.000000\n",
      "step 70000, training accuracy 1.000000\n",
      "step 71000, training accuracy 1.000000\n",
      "step 72000, training accuracy 1.000000\n",
      "step 73000, training accuracy 1.000000\n",
      "step 74000, training accuracy 1.000000\n",
      "step 75000, training accuracy 1.000000\n",
      "step 76000, training accuracy 1.000000\n",
      "step 77000, training accuracy 1.000000\n",
      "step 78000, training accuracy 1.000000\n",
      "step 79000, training accuracy 1.000000\n",
      "step 80000, training accuracy 1.000000\n",
      "step 81000, training accuracy 1.000000\n",
      "step 82000, training accuracy 1.000000\n",
      "step 83000, training accuracy 1.000000\n",
      "step 84000, training accuracy 1.000000\n",
      "step 85000, training accuracy 1.000000\n",
      "step 86000, training accuracy 1.000000\n",
      "step 87000, training accuracy 1.000000\n",
      "step 88000, training accuracy 1.000000\n",
      "step 89000, training accuracy 1.000000\n",
      "step 90000, training accuracy 1.000000\n",
      "step 91000, training accuracy 1.000000\n",
      "step 92000, training accuracy 1.000000\n",
      "step 93000, training accuracy 1.000000\n",
      "step 94000, training accuracy 1.000000\n",
      "step 95000, training accuracy 1.000000\n",
      "step 96000, training accuracy 1.000000\n",
      "step 97000, training accuracy 1.000000\n",
      "step 98000, training accuracy 1.000000\n",
      "step 99000, training accuracy 1.000000\n",
      "test accuracy of the student model is 0.983600\n"
     ]
    }
   ],
   "source": [
    "test_student_only_acc = runMNIST(scope='student', num_hidden_units=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ySOtUInyO1LN"
   },
   "source": [
    "#### 4.3 Results and Limitations\n",
    "Our replication results align with the main point of the paper: while the cumbersome model achieves the high test accuracy, distilled student network with soft target performs better than undistilled student network with hard targets. \n",
    "\n",
    "The results we got are:\n",
    "<ul>\n",
    "    <li> Test accuracy of teacher network: <b>98.76%</b> (<b>124</b> errors in 10000 test samples).</li>\n",
    "    <li> Test accuracy of distilled student network: <b>98.59%</b> (<b>141</b> errors in 10000 test samples).</li>\n",
    "     <li> Test accuracy of undistilled student network: <b>98.36%</b> (<b>164</b> errors in 10000 test samples).</li>\n",
    "</ul>\n",
    "\n",
    "Under such pararmeter setting, we could not exactly replicate the results presented in paper, `teacher network produces 67 test errors, undistilled student network achieves 146 errors and distilled student network achieves 74 test errors`. This is partly because we do not know the best choice for lots of hyperparameters, such as $\\lambda$ (the weight we assign to hard target function), the learning rate, the batch size, etc. In addition, based on the computation resource we have, it took too much time to do cross-validation.\n",
    "\n",
    "However, as seen from the results above, we successfully showed that the distilled student network performs better than undistilled student network (the model with hard targets) on test dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTQZ4nMACq2G"
   },
   "source": [
    "#### 4.4 Distillation on Incomplete MNIST\n",
    "As mentioned on paper, we can remove digit 3 in the training set to see whether distillation can help capture underlying information in soft targets, which should make model give better performance while testing. \n",
    "\n",
    "The results we got are:\n",
    "<ul>\n",
    "    <li> Test accuracy of teacher network: <b>88.73%</b>.</li>\n",
    "    <li> Test accuracy of distilled student network: <b>88.69%</b>.</li>\n",
    "     <li> Test accuracy of undistilled student network: <b>88.56%</b>.</li>\n",
    "</ul>\n",
    "\n",
    "Again, distilled student generalize better to unseen data, digit 3 class, than undistilled student network.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzctsakTCjmr"
   },
   "outputs": [],
   "source": [
    "def batch_idx(b, n, i):\n",
    "    if b >= n:\n",
    "        return np.arange(n)\n",
    "    idx1 = (i*b)%n\n",
    "    idx2 = ((i+1)*b)%n\n",
    "    if idx2 < idx1:\n",
    "        idx1 -= n\n",
    "    return np.arange(idx1, idx2)\n",
    "    \n",
    "def runDistillationMNISTWithoutDigits(numbers, verbose=True, print_every=1000, teacher_iters=30000, student_iters=100000, \\\n",
    "                         batch_size=128, lr=0.4, teacher_num_units=1200, student_num_units=800, \\\n",
    "                         temperature=10, lam=10, keep_prob=0.5, l=15):\n",
    "    with tf.Graph().as_default():\n",
    "        # load data\n",
    "        mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "        train_images = np.array([x for (x,y) in zip(mnist.train.images, mnist.train.labels) if np.all(y[numbers]==0)])\n",
    "        train_labels = np.array([y for (x,y) in zip(mnist.train.images, mnist.train.labels) if np.all(y[numbers]==0)])\n",
    "        n = train_images.shape[0]\n",
    "\n",
    "        # placeholder\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "        dropout_rate = tf.placeholder(tf.float32)\n",
    "\n",
    "        # network output and divided by t\n",
    "        y_teacher = FCNetwork(x, dropout_rate, teacher_num_units, scope='teacher')\n",
    "        y_student = FCNetwork(x, num_units=student_num_units, scope='student')\n",
    "        y_teacher_t = y_teacher/temperature\n",
    "        y_student_t = y_student/temperature\n",
    "\n",
    "        # loss and acc\n",
    "        cross_entropy_teacher, accuracy_teacher = loss(y_teacher, y_)\n",
    "        student_loss1, _ = loss(y_student_t, tf.nn.softmax(y_teacher_t))\n",
    "        student_loss2, accuracy_student = loss(y_student, y_)\n",
    "        cross_entropy_student = (temperature**2 * lam * student_loss1 + student_loss2) / (temperature**2 * lam + 1)\n",
    "\n",
    "        # get vars and gradients\n",
    "        model_vars = tf.trainable_variables()\n",
    "        var_teacher = [var for var in model_vars if 'teacher' in var.name]\n",
    "        var_student = [var for var in model_vars if 'student' in var.name]\n",
    "        grad_teacher = tf.gradients(cross_entropy_teacher,var_teacher)\n",
    "        \n",
    "        # add coonstraints on teacher nn\n",
    "        grad_teacher = [tf.clip_by_norm(g, clip_norm=l, axes=0) for g in grad_teacher] \n",
    "        grad_student = tf.gradients(cross_entropy_student,var_student)\n",
    "\n",
    "        # use gradient descent\n",
    "        trainer_teacher = tf.train.GradientDescentOptimizer(lr)\n",
    "        trainer_student = tf.train.GradientDescentOptimizer(lr)\n",
    "        step_teacher = trainer_teacher.apply_gradients(zip(grad_teacher,var_teacher))\n",
    "        step_student = trainer_student.apply_gradients(zip(grad_student,var_student))\n",
    "\n",
    "        # sess\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # train and test teacher network\n",
    "        for i in range(teacher_iters):\n",
    "            batch = batch_idx(batch_size, n, i)\n",
    "            batch_x = train_images[batch,:]\n",
    "            batch_y = train_labels[batch,:]\n",
    "            if i% print_every == 0:\n",
    "                train_acc_teacher = accuracy_teacher.eval(feed_dict={x: batch_x, y_: batch_y, dropout_rate: 1.0})\n",
    "                print(\"step %d, training accuracy %f\"%(i, train_acc_teacher))\n",
    "            step_teacher.run(feed_dict={x: batch_x, y_: batch_y, dropout_rate: keep_prob})\n",
    "\n",
    "        test_acc_teacher = sess.run(accuracy_teacher, feed_dict={x: mnist.test.images, y_: mnist.test.labels, dropout_rate: 1.0})\n",
    "        print(\"test accuracy of the teacher model is %f\"%(test_acc_teacher))\n",
    "\n",
    "        # train and test student network\n",
    "        for i in range(student_iters):\n",
    "            batch = batch_idx(batch_size, n, i)\n",
    "            batch_x = train_images[batch,:]\n",
    "            batch_y = train_labels[batch,:]\n",
    "            if i% print_every == 0:\n",
    "                train_acc_student = accuracy_student.eval(feed_dict={x: batch_x, y_: batch_y, dropout_rate: 1.0})\n",
    "                print(\"step %d, training accuracy %f\"%(i, train_acc_student))\n",
    "            step_student.run(feed_dict={x: batch_x, y_: batch_y, dropout_rate: 1.0}) \n",
    "\n",
    "        test_acc_student = sess.run(accuracy_student, feed_dict={x:mnist.test.images, y_: mnist.test.labels, dropout_rate: 1.0})\n",
    "        print(\"test accuracy of the student model is %f\"%(test_acc_student))\n",
    "        \n",
    "        return test_acc_teacher, test_acc_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2774
    },
    "colab_type": "code",
    "id": "PQhNEwcYIUwg",
    "outputId": "78ff4a8b-5857-4626-8d87-3cead03c3b1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-a71a37092c8c>:13: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "step 0, training accuracy 0.031250\n",
      "step 1000, training accuracy 0.976562\n",
      "step 2000, training accuracy 0.992188\n",
      "step 3000, training accuracy 1.000000\n",
      "step 4000, training accuracy 0.984375\n",
      "step 5000, training accuracy 1.000000\n",
      "step 6000, training accuracy 1.000000\n",
      "step 7000, training accuracy 1.000000\n",
      "step 8000, training accuracy 0.992188\n",
      "step 9000, training accuracy 1.000000\n",
      "step 10000, training accuracy 1.000000\n",
      "step 11000, training accuracy 1.000000\n",
      "step 12000, training accuracy 1.000000\n",
      "step 13000, training accuracy 1.000000\n",
      "step 14000, training accuracy 1.000000\n",
      "step 15000, training accuracy 1.000000\n",
      "step 16000, training accuracy 1.000000\n",
      "step 17000, training accuracy 1.000000\n",
      "step 18000, training accuracy 1.000000\n",
      "step 19000, training accuracy 1.000000\n",
      "step 20000, training accuracy 1.000000\n",
      "step 21000, training accuracy 1.000000\n",
      "step 22000, training accuracy 1.000000\n",
      "step 23000, training accuracy 1.000000\n",
      "step 24000, training accuracy 1.000000\n",
      "step 25000, training accuracy 1.000000\n",
      "step 26000, training accuracy 1.000000\n",
      "step 27000, training accuracy 1.000000\n",
      "step 28000, training accuracy 1.000000\n",
      "step 29000, training accuracy 1.000000\n",
      "test accuracy of the teacher model is 0.887300\n",
      "step 0, training accuracy 0.117188\n",
      "step 1000, training accuracy 0.937500\n",
      "step 2000, training accuracy 0.953125\n",
      "step 3000, training accuracy 0.960938\n",
      "step 4000, training accuracy 0.968750\n",
      "step 5000, training accuracy 0.992188\n",
      "step 6000, training accuracy 0.992188\n",
      "step 7000, training accuracy 0.976562\n",
      "step 8000, training accuracy 0.984375\n",
      "step 9000, training accuracy 1.000000\n",
      "step 10000, training accuracy 1.000000\n",
      "step 11000, training accuracy 1.000000\n",
      "step 12000, training accuracy 1.000000\n",
      "step 13000, training accuracy 1.000000\n",
      "step 14000, training accuracy 1.000000\n",
      "step 15000, training accuracy 1.000000\n",
      "step 16000, training accuracy 1.000000\n",
      "step 17000, training accuracy 0.992188\n",
      "step 18000, training accuracy 1.000000\n",
      "step 19000, training accuracy 0.992188\n",
      "step 20000, training accuracy 1.000000\n",
      "step 21000, training accuracy 0.992188\n",
      "step 22000, training accuracy 1.000000\n",
      "step 23000, training accuracy 1.000000\n",
      "step 24000, training accuracy 1.000000\n",
      "step 25000, training accuracy 1.000000\n",
      "step 26000, training accuracy 0.992188\n",
      "step 27000, training accuracy 0.992188\n",
      "step 28000, training accuracy 1.000000\n",
      "step 29000, training accuracy 0.992188\n",
      "step 30000, training accuracy 1.000000\n",
      "step 31000, training accuracy 1.000000\n",
      "step 32000, training accuracy 1.000000\n",
      "step 33000, training accuracy 1.000000\n",
      "step 34000, training accuracy 1.000000\n",
      "step 35000, training accuracy 1.000000\n",
      "step 36000, training accuracy 1.000000\n",
      "step 37000, training accuracy 1.000000\n",
      "step 38000, training accuracy 1.000000\n",
      "step 39000, training accuracy 1.000000\n",
      "step 40000, training accuracy 1.000000\n",
      "step 41000, training accuracy 1.000000\n",
      "step 42000, training accuracy 1.000000\n",
      "step 43000, training accuracy 1.000000\n",
      "step 44000, training accuracy 1.000000\n",
      "step 45000, training accuracy 1.000000\n",
      "step 46000, training accuracy 1.000000\n",
      "step 47000, training accuracy 1.000000\n",
      "step 48000, training accuracy 1.000000\n",
      "step 49000, training accuracy 0.992188\n",
      "step 50000, training accuracy 1.000000\n",
      "step 51000, training accuracy 1.000000\n",
      "step 52000, training accuracy 1.000000\n",
      "step 53000, training accuracy 1.000000\n",
      "step 54000, training accuracy 1.000000\n",
      "step 55000, training accuracy 1.000000\n",
      "step 56000, training accuracy 1.000000\n",
      "step 57000, training accuracy 1.000000\n",
      "step 58000, training accuracy 1.000000\n",
      "step 59000, training accuracy 1.000000\n",
      "step 60000, training accuracy 1.000000\n",
      "step 61000, training accuracy 1.000000\n",
      "step 62000, training accuracy 1.000000\n",
      "step 63000, training accuracy 1.000000\n",
      "step 64000, training accuracy 1.000000\n",
      "step 65000, training accuracy 1.000000\n",
      "step 66000, training accuracy 0.992188\n",
      "step 67000, training accuracy 1.000000\n",
      "step 68000, training accuracy 1.000000\n",
      "step 69000, training accuracy 1.000000\n",
      "step 70000, training accuracy 1.000000\n",
      "step 71000, training accuracy 1.000000\n",
      "step 72000, training accuracy 1.000000\n",
      "step 73000, training accuracy 1.000000\n",
      "step 74000, training accuracy 1.000000\n",
      "step 75000, training accuracy 1.000000\n",
      "step 76000, training accuracy 1.000000\n",
      "step 77000, training accuracy 1.000000\n",
      "step 78000, training accuracy 1.000000\n",
      "step 79000, training accuracy 1.000000\n",
      "step 80000, training accuracy 1.000000\n",
      "step 81000, training accuracy 1.000000\n",
      "step 82000, training accuracy 1.000000\n",
      "step 83000, training accuracy 1.000000\n",
      "step 84000, training accuracy 1.000000\n",
      "step 85000, training accuracy 1.000000\n",
      "step 86000, training accuracy 1.000000\n",
      "step 87000, training accuracy 1.000000\n",
      "step 88000, training accuracy 1.000000\n",
      "step 89000, training accuracy 1.000000\n",
      "step 90000, training accuracy 1.000000\n",
      "step 91000, training accuracy 1.000000\n",
      "step 92000, training accuracy 1.000000\n",
      "step 93000, training accuracy 1.000000\n",
      "step 94000, training accuracy 1.000000\n",
      "step 95000, training accuracy 1.000000\n",
      "step 96000, training accuracy 0.992188\n",
      "step 97000, training accuracy 1.000000\n",
      "step 98000, training accuracy 1.000000\n",
      "step 99000, training accuracy 1.000000\n",
      "test accuracy of the student model is 0.886900\n"
     ]
    }
   ],
   "source": [
    "test_acc_teacher1, test_acc_student1 = runDistillationMNISTWithoutDigits([3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGrg_41cIhX9"
   },
   "outputs": [],
   "source": [
    "def runMNISTWithoutDigits(numbers, verbose=True, print_every=1000, iters=100000, scope='teacher', num_hidden_units=1200, keep_prob=0.5, batch_size=128, lr=0.4):\n",
    "    with tf.Graph().as_default():\n",
    "        # load data\n",
    "        mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "        train_images = np.array([x for (x,y) in zip(mnist.train.images, mnist.train.labels) if np.all(y[numbers]==0)])\n",
    "        train_labels = np.array([y for (x,y) in zip(mnist.train.images, mnist.train.labels) if np.all(y[numbers]==0)])\n",
    "        n = train_images.shape[0]\n",
    "\n",
    "        # placeholder\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "        # network output and divided by t       \n",
    "        y = FCNetwork(x, keep_prob, num_hidden_units, scope=scope)\n",
    " \n",
    "        # loss and acc\n",
    "        cross_entropy, accuracy = loss(y, y_)\n",
    "\n",
    "        # use gradient descent to optimize\n",
    "        train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "        # sess\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # train and test student network\n",
    "        for i in range(iters):\n",
    "            batch = batch_idx(batch_size, n, i)\n",
    "            batch_x = train_images[batch,:]\n",
    "            batch_y = train_labels[batch,:]\n",
    "            if i% print_every == 0:\n",
    "                train_acc = accuracy.eval(feed_dict={x:batch_x, y_: batch_y})\n",
    "                print(\"step %d, training accuracy %f\"%(i, train_acc))\n",
    "            train_step.run(feed_dict={x: batch_x, y_: batch_y}) \n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict={x:mnist.test.images, y_: mnist.test.labels})\n",
    "        print(\"test accuracy of the student model is %f\"%(test_acc))\n",
    "        \n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1856
    },
    "colab_type": "code",
    "id": "hg-sWfo2L7y2",
    "outputId": "34fe0312-749c-42d2-cae8-82df4b0ade32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.070312\n",
      "step 1000, training accuracy 0.968750\n",
      "step 2000, training accuracy 1.000000\n",
      "step 3000, training accuracy 1.000000\n",
      "step 4000, training accuracy 1.000000\n",
      "step 5000, training accuracy 1.000000\n",
      "step 6000, training accuracy 1.000000\n",
      "step 7000, training accuracy 1.000000\n",
      "step 8000, training accuracy 1.000000\n",
      "step 9000, training accuracy 1.000000\n",
      "step 10000, training accuracy 1.000000\n",
      "step 11000, training accuracy 1.000000\n",
      "step 12000, training accuracy 1.000000\n",
      "step 13000, training accuracy 1.000000\n",
      "step 14000, training accuracy 1.000000\n",
      "step 15000, training accuracy 1.000000\n",
      "step 16000, training accuracy 1.000000\n",
      "step 17000, training accuracy 1.000000\n",
      "step 18000, training accuracy 1.000000\n",
      "step 19000, training accuracy 1.000000\n",
      "step 20000, training accuracy 1.000000\n",
      "step 21000, training accuracy 1.000000\n",
      "step 22000, training accuracy 1.000000\n",
      "step 23000, training accuracy 1.000000\n",
      "step 24000, training accuracy 1.000000\n",
      "step 25000, training accuracy 1.000000\n",
      "step 26000, training accuracy 1.000000\n",
      "step 27000, training accuracy 1.000000\n",
      "step 28000, training accuracy 1.000000\n",
      "step 29000, training accuracy 1.000000\n",
      "step 30000, training accuracy 1.000000\n",
      "step 31000, training accuracy 1.000000\n",
      "step 32000, training accuracy 1.000000\n",
      "step 33000, training accuracy 1.000000\n",
      "step 34000, training accuracy 1.000000\n",
      "step 35000, training accuracy 1.000000\n",
      "step 36000, training accuracy 1.000000\n",
      "step 37000, training accuracy 1.000000\n",
      "step 38000, training accuracy 1.000000\n",
      "step 39000, training accuracy 1.000000\n",
      "step 40000, training accuracy 1.000000\n",
      "step 41000, training accuracy 1.000000\n",
      "step 42000, training accuracy 1.000000\n",
      "step 43000, training accuracy 1.000000\n",
      "step 44000, training accuracy 1.000000\n",
      "step 45000, training accuracy 1.000000\n",
      "step 46000, training accuracy 1.000000\n",
      "step 47000, training accuracy 1.000000\n",
      "step 48000, training accuracy 1.000000\n",
      "step 49000, training accuracy 1.000000\n",
      "step 50000, training accuracy 1.000000\n",
      "step 51000, training accuracy 1.000000\n",
      "step 52000, training accuracy 1.000000\n",
      "step 53000, training accuracy 1.000000\n",
      "step 54000, training accuracy 1.000000\n",
      "step 55000, training accuracy 1.000000\n",
      "step 56000, training accuracy 1.000000\n",
      "step 57000, training accuracy 1.000000\n",
      "step 58000, training accuracy 1.000000\n",
      "step 59000, training accuracy 1.000000\n",
      "step 60000, training accuracy 1.000000\n",
      "step 61000, training accuracy 1.000000\n",
      "step 62000, training accuracy 1.000000\n",
      "step 63000, training accuracy 1.000000\n",
      "step 64000, training accuracy 1.000000\n",
      "step 65000, training accuracy 1.000000\n",
      "step 66000, training accuracy 1.000000\n",
      "step 67000, training accuracy 1.000000\n",
      "step 68000, training accuracy 1.000000\n",
      "step 69000, training accuracy 1.000000\n",
      "step 70000, training accuracy 1.000000\n",
      "step 71000, training accuracy 1.000000\n",
      "step 72000, training accuracy 1.000000\n",
      "step 73000, training accuracy 1.000000\n",
      "step 74000, training accuracy 1.000000\n",
      "step 75000, training accuracy 1.000000\n",
      "step 76000, training accuracy 1.000000\n",
      "step 77000, training accuracy 1.000000\n",
      "step 78000, training accuracy 1.000000\n",
      "step 79000, training accuracy 1.000000\n",
      "step 80000, training accuracy 1.000000\n",
      "step 81000, training accuracy 1.000000\n",
      "step 82000, training accuracy 1.000000\n",
      "step 83000, training accuracy 1.000000\n",
      "step 84000, training accuracy 1.000000\n",
      "step 85000, training accuracy 1.000000\n",
      "step 86000, training accuracy 1.000000\n",
      "step 87000, training accuracy 1.000000\n",
      "step 88000, training accuracy 1.000000\n",
      "step 89000, training accuracy 1.000000\n",
      "step 90000, training accuracy 1.000000\n",
      "step 91000, training accuracy 1.000000\n",
      "step 92000, training accuracy 1.000000\n",
      "step 93000, training accuracy 1.000000\n",
      "step 94000, training accuracy 1.000000\n",
      "step 95000, training accuracy 1.000000\n",
      "step 96000, training accuracy 1.000000\n",
      "step 97000, training accuracy 1.000000\n",
      "step 98000, training accuracy 1.000000\n",
      "step 99000, training accuracy 1.000000\n",
      "test accuracy of the student model is 0.885600\n"
     ]
    }
   ],
   "source": [
    "test_student_only1 = runMNISTWithoutDigits([3], scope='student', num_hidden_units=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gEuvfP8t7vEJ"
   },
   "source": [
    "### 5. Implementations of Knowledge Distillation on CIFAR-10\n",
    "#### 5.1 Reinvented LeNet-5\n",
    "\n",
    "We ran similar tests to CIFAR-10 datasets. First, in order to test distilliation on CIFAR-10 dataset, we utilized Tensorflow to implement simplest convolution neural network - LeNet-5 as the teacher model. Here we reinvent the architecture of LeNet for better demonstration of knowledge distillation. Specifically, we substitute the second last fully-connected layer in the original LeNet with the feed-forward network (two hidden layers) we use in MNIST test. The number of hidden units in each fully-connected layer is still set to 1200. We continue using dropout with $p=0.5$ for regularization but discard the constriant on weights because it's hard to cross-validate. Besides, we add ReLU for all layers in network except the output layer.\n",
    "\n",
    "The reinvented network architechture is shown below:\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th colspan=2>Layer 1</th>\n",
    "            <th>Feature Map</th>\n",
    "            <th>Size</th>\n",
    "            <th>Kernel Size</th>\n",
    "            <th>Stride</th>\n",
    "            <th>Activation</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Input</td>\n",
    "            <td>Image</td>\n",
    "            <td>1</td>\n",
    "            <td>32x32</td>\n",
    "            <td>-</td>\n",
    "            <td>-</td>\n",
    "             <td>-</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>1</td>\n",
    "            <td>Convolution</td>\n",
    "            <td>6</td>\n",
    "            <td>28x28</td>\n",
    "            <td>5x5</td>\n",
    "            <td>1</td>\n",
    "             <td>ReLU</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>2</td>\n",
    "            <td>Pooling</td>\n",
    "            <td>6</td>\n",
    "            <td>14x14</td>\n",
    "            <td>2x2</td>\n",
    "            <td>2</td>\n",
    "             <td>ReLU</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>3</td>\n",
    "            <td>Convolution</td>\n",
    "            <td>16</td>\n",
    "            <td>10x10</td>\n",
    "            <td>5x5</td>\n",
    "            <td>1</td>\n",
    "             <td>ReLU</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>4</td>\n",
    "            <td>Pooling</td>\n",
    "            <td>16</td>\n",
    "            <td>5x5</td>\n",
    "            <td>2x2</td>\n",
    "            <td>2</td>\n",
    "             <td>ReLU</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>5</td>\n",
    "            <td>Convolution</td>\n",
    "            <td>120</td>\n",
    "            <td>1x1</td>\n",
    "            <td>5x5</td>\n",
    "            <td>1</td>\n",
    "             <td>ReLU</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>6</td>\n",
    "            <td>Fully-Connected</td>\n",
    "            <td>-</td>\n",
    "            <td>1200</td>\n",
    "            <td>-</td>\n",
    "            <td>-</td>\n",
    "             <td>ReLU</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>7</td>\n",
    "            <td>Fully-Connected</td>\n",
    "            <td>-</td>\n",
    "            <td>1200</td>\n",
    "            <td>-</td>\n",
    "            <td>-</td>\n",
    "             <td>ReLU</td>\n",
    "        </tr>\n",
    "         <tr>\n",
    "            <td>Output</td>\n",
    "            <td>Fully-Connected</td>\n",
    "            <td>-</td>\n",
    "            <td>10</td>\n",
    "            <td>-</td>\n",
    "            <td>-</td>\n",
    "             <td>Softmax</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "We still used simple fully-connected neural networks with 800 units in two hidden layers  as the student model and no regularization applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eWk-sIqMU1o9"
   },
   "outputs": [],
   "source": [
    "def LeNet(input, dropout_rate=0.5, num_units=1200, scope='teacher',reuse = False):\n",
    "    with tf.variable_scope(scope,reuse = reuse) as sc :\n",
    "        with slim.arg_scope([slim.conv2d], padding='VALID',\n",
    "                            activation_fn=tf.nn.relu,\n",
    "                            weights_initializer=tf.truncated_normal_initializer(stddev=0.01),\n",
    "                            weights_regularizer=slim.l2_regularizer(0.0005)):\n",
    "            net = slim.conv2d(input,6,[5,5],1,padding='SAME',scope='conv1')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "            net = slim.conv2d(net,16,[5,5],1,scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool4')\n",
    "            net = slim.conv2d(net,120,[5,5],1,scope='conv5')\n",
    "            net = slim.flatten(net, scope='flat6')\n",
    "            net = slim.fully_connected(net, num_units, scope='fc7')\n",
    "            if scope == 'teacher':\n",
    "                net = tf.nn.dropout(net, dropout_rate)\n",
    "            net = slim.fully_connected(net, num_units, scope='fc8')\n",
    "            if scope == 'teacher':\n",
    "                net = tf.nn.dropout(net, dropout_rate)\n",
    "            net = slim.fully_connected(net, 10, activation_fn=None, scope='fc9')\n",
    "            return net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l7iCiD_d-PLe"
   },
   "source": [
    "#### 5.2 Training, Test, and Distillation on CIFAR-10\n",
    "We still adopted gradient descent to optimize the model but set learning rate to 0.04. We ran 75k iterations for teacher network, 300k iterations for distilled student network and 100k iterations for undistilled student network (obviously converged at this point). The batch size is set to 128 for both teacher and student networks. While traning both the cumbersome model and the smaller model, the tempature is set to 5 by cross-validation. Again, we used same strategy to assign considerably low weight for hard target function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWiq0JH5Qt-i"
   },
   "outputs": [],
   "source": [
    "def runDistillationCIFAR(verbose=True, print_every=500, teacher_iters=75000, student_iters=300000, \\\n",
    "                         batch_size=128, lr=0.04, teacher_num_units=1200, student_num_units=800, \\\n",
    "                         temperature=5, lam=10, keep_prob=0.5):\n",
    "    with tf.Graph().as_default():\n",
    "        \n",
    "        #load data\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        \n",
    "        # one-hot encode the labels\n",
    "        y_train = keras.utils.np_utils.to_categorical(y_train)\n",
    "        y_test = keras.utils.np_utils.to_categorical(y_test)\n",
    "        \n",
    "        # scale the data \n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "        n = x_train.shape[0]\n",
    "        \n",
    "        # placeholder\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "        dropout_rate = tf.placeholder(tf.float32)\n",
    "        x_flat = tf.contrib.layers.flatten(x)\n",
    "        \n",
    "        # network output and divided by t\n",
    "        y_teacher = LeNet(x, dropout_rate, teacher_num_units, scope='teacher')\n",
    "        y_student = FCNetwork(x_flat, num_units=student_num_units, scope='student')\n",
    "        y_teacher_t = y_teacher/temperature\n",
    "        y_student_t = y_student/temperature\n",
    "        \n",
    "        # loss and acc\n",
    "        cross_entropy_teacher, accuracy_teacher = loss(y_teacher, y_)\n",
    "        student_loss1, _ = loss(y_student_t, tf.nn.softmax(y_teacher_t))\n",
    "        student_loss2, accuracy_student = loss(y_student, y_)\n",
    "        cross_entropy_student = (temperature**2 * lam * student_loss1 + student_loss2) / (temperature**2 * lam + 1)\n",
    "        \n",
    "        # get vars and gradients\n",
    "        model_vars = tf.trainable_variables()\n",
    "        var_teacher = [var for var in model_vars if 'teacher' in var.name]\n",
    "        var_student = [var for var in model_vars if 'student' in var.name]\n",
    "        grad_teacher = tf.gradients(cross_entropy_teacher,var_teacher)\n",
    "        grad_student = tf.gradients(cross_entropy_student,var_student)\n",
    "        \n",
    "        # use gradient descent\n",
    "        trainer_teacher = tf.train.GradientDescentOptimizer(lr)\n",
    "        trainer_student = tf.train.GradientDescentOptimizer(lr)\n",
    "        step_teacher = trainer_teacher.apply_gradients(zip(grad_teacher,var_teacher))\n",
    "        step_student = trainer_student.apply_gradients(zip(grad_student,var_student))\n",
    "        \n",
    "        # sess\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # train and test teacher network\n",
    "        for i in range(teacher_iters):\n",
    "            batch = batch_idx(batch_size, n, i)\n",
    "            batch_x = x_train[batch,:]\n",
    "            batch_y = y_train[batch,:]\n",
    "            if i% print_every == 0:\n",
    "                train_acc_teacher = accuracy_teacher.eval(feed_dict={x:batch_x, y_: batch_y,dropout_rate: 1.0})\n",
    "                print(\"step %d, training accuracy %f\"%(i, train_acc_teacher))\n",
    "            step_teacher.run(feed_dict={x: batch_x, y_: batch_y, dropout_rate: keep_prob})\n",
    "\n",
    "        test_acc_teacher = sess.run(accuracy_teacher, feed_dict={x:x_test, y_: y_test, dropout_rate: 1.0})\n",
    "        print(\"test accuracy of the teacher model is %f\"%(test_acc_teacher))\n",
    "        \n",
    "        # train and test student network\n",
    "        for i in range(student_iters):\n",
    "            batch = batch_idx(batch_size, n, i)\n",
    "            batch_x = x_train[batch,:]\n",
    "            batch_y = y_train[batch,:]\n",
    "            if i% print_every == 0:\n",
    "                train_acc_student = accuracy_student.eval(feed_dict={x:batch_x, y_: batch_y,dropout_rate: 1.0})\n",
    "                print(\"step %d, training accuracy %f\"%(i, train_acc_student))\n",
    "            step_student.run(feed_dict={x: batch_x, y_: batch_y,dropout_rate: 1.0}) \n",
    "\n",
    "        test_acc_student = sess.run(accuracy_student, feed_dict={x:x_test, y_: y_test, dropout_rate: 1.0})\n",
    "        print(\"test accuracy of the student model is %f\"%(test_acc_student))\n",
    "        \n",
    "        return test_acc_teacher, test_acc_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 12855
    },
    "colab_type": "code",
    "id": "UB_yf5Ss15cg",
    "outputId": "d485404b-fb9e-4962-bc58-197ae5627c89"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.125000\n",
      "step 500, training accuracy 0.125000\n",
      "step 1000, training accuracy 0.148438\n",
      "step 1500, training accuracy 0.218750\n",
      "step 2000, training accuracy 0.335938\n",
      "step 2500, training accuracy 0.351562\n",
      "step 3000, training accuracy 0.382812\n",
      "step 3500, training accuracy 0.281250\n",
      "step 4000, training accuracy 0.367188\n",
      "step 4500, training accuracy 0.476562\n",
      "step 5000, training accuracy 0.531250\n",
      "step 5500, training accuracy 0.453125\n",
      "step 6000, training accuracy 0.585938\n",
      "step 6500, training accuracy 0.507812\n",
      "step 7000, training accuracy 0.593750\n",
      "step 7500, training accuracy 0.625000\n",
      "step 8000, training accuracy 0.578125\n",
      "step 8500, training accuracy 0.554688\n",
      "step 9000, training accuracy 0.570312\n",
      "step 9500, training accuracy 0.640625\n",
      "step 10000, training accuracy 0.632812\n",
      "step 10500, training accuracy 0.585938\n",
      "step 11000, training accuracy 0.601562\n",
      "step 11500, training accuracy 0.578125\n",
      "step 12000, training accuracy 0.671875\n",
      "step 12500, training accuracy 0.648438\n",
      "step 13000, training accuracy 0.648438\n",
      "step 13500, training accuracy 0.703125\n",
      "step 14000, training accuracy 0.703125\n",
      "step 14500, training accuracy 0.687500\n",
      "step 15000, training accuracy 0.710938\n",
      "step 15500, training accuracy 0.632812\n",
      "step 16000, training accuracy 0.687500\n",
      "step 16500, training accuracy 0.750000\n",
      "step 17000, training accuracy 0.703125\n",
      "step 17500, training accuracy 0.789062\n",
      "step 18000, training accuracy 0.718750\n",
      "step 18500, training accuracy 0.757812\n",
      "step 19000, training accuracy 0.742188\n",
      "step 19500, training accuracy 0.757812\n",
      "step 20000, training accuracy 0.843750\n",
      "step 20500, training accuracy 0.765625\n",
      "step 21000, training accuracy 0.703125\n",
      "step 21500, training accuracy 0.804688\n",
      "step 22000, training accuracy 0.843750\n",
      "step 22500, training accuracy 0.804688\n",
      "step 23000, training accuracy 0.859375\n",
      "step 23500, training accuracy 0.757812\n",
      "step 24000, training accuracy 0.804688\n",
      "step 24500, training accuracy 0.835938\n",
      "step 25000, training accuracy 0.875000\n",
      "step 25500, training accuracy 0.820312\n",
      "step 26000, training accuracy 0.828125\n",
      "step 26500, training accuracy 0.812500\n",
      "step 27000, training accuracy 0.867188\n",
      "step 27500, training accuracy 0.867188\n",
      "step 28000, training accuracy 0.812500\n",
      "step 28500, training accuracy 0.828125\n",
      "step 29000, training accuracy 0.906250\n",
      "step 29500, training accuracy 0.921875\n",
      "step 30000, training accuracy 0.914062\n",
      "step 30500, training accuracy 0.906250\n",
      "step 31000, training accuracy 0.960938\n",
      "step 31500, training accuracy 0.914062\n",
      "step 32000, training accuracy 0.929688\n",
      "step 32500, training accuracy 0.968750\n",
      "step 33000, training accuracy 0.898438\n",
      "step 33500, training accuracy 0.921875\n",
      "step 34000, training accuracy 0.937500\n",
      "step 34500, training accuracy 0.937500\n",
      "step 35000, training accuracy 0.953125\n",
      "step 35500, training accuracy 0.937500\n",
      "step 36000, training accuracy 0.937500\n",
      "step 36500, training accuracy 0.976562\n",
      "step 37000, training accuracy 0.968750\n",
      "step 37500, training accuracy 0.945312\n",
      "step 38000, training accuracy 0.976562\n",
      "step 38500, training accuracy 0.984375\n",
      "step 39000, training accuracy 0.976562\n",
      "step 39500, training accuracy 0.992188\n",
      "step 40000, training accuracy 0.976562\n",
      "step 40500, training accuracy 0.976562\n",
      "step 41000, training accuracy 0.984375\n",
      "step 41500, training accuracy 0.976562\n",
      "step 42000, training accuracy 0.984375\n",
      "step 42500, training accuracy 0.984375\n",
      "step 43000, training accuracy 0.984375\n",
      "step 43500, training accuracy 0.992188\n",
      "step 44000, training accuracy 0.976562\n",
      "step 44500, training accuracy 0.976562\n",
      "step 45000, training accuracy 0.984375\n",
      "step 45500, training accuracy 0.992188\n",
      "step 46000, training accuracy 1.000000\n",
      "step 46500, training accuracy 1.000000\n",
      "step 47000, training accuracy 0.992188\n",
      "step 47500, training accuracy 0.992188\n",
      "step 48000, training accuracy 1.000000\n",
      "step 48500, training accuracy 0.984375\n",
      "step 49000, training accuracy 0.992188\n",
      "step 49500, training accuracy 0.992188\n",
      "step 50000, training accuracy 1.000000\n",
      "step 50500, training accuracy 1.000000\n",
      "step 51000, training accuracy 1.000000\n",
      "step 51500, training accuracy 0.992188\n",
      "step 52000, training accuracy 1.000000\n",
      "step 52500, training accuracy 0.992188\n",
      "step 53000, training accuracy 1.000000\n",
      "step 53500, training accuracy 0.984375\n",
      "step 54000, training accuracy 0.992188\n",
      "step 54500, training accuracy 1.000000\n",
      "step 55000, training accuracy 0.992188\n",
      "step 55500, training accuracy 1.000000\n",
      "step 56000, training accuracy 1.000000\n",
      "step 56500, training accuracy 1.000000\n",
      "step 57000, training accuracy 0.992188\n",
      "step 57500, training accuracy 1.000000\n",
      "step 58000, training accuracy 1.000000\n",
      "step 58500, training accuracy 1.000000\n",
      "step 59000, training accuracy 1.000000\n",
      "step 59500, training accuracy 1.000000\n",
      "step 60000, training accuracy 1.000000\n",
      "step 60500, training accuracy 1.000000\n",
      "step 61000, training accuracy 1.000000\n",
      "step 61500, training accuracy 1.000000\n",
      "step 62000, training accuracy 0.992188\n",
      "step 62500, training accuracy 1.000000\n",
      "step 63000, training accuracy 1.000000\n",
      "step 63500, training accuracy 1.000000\n",
      "step 64000, training accuracy 1.000000\n",
      "step 64500, training accuracy 0.992188\n",
      "step 65000, training accuracy 1.000000\n",
      "step 65500, training accuracy 1.000000\n",
      "step 66000, training accuracy 1.000000\n",
      "step 66500, training accuracy 1.000000\n",
      "step 67000, training accuracy 0.992188\n",
      "step 67500, training accuracy 1.000000\n",
      "step 68000, training accuracy 1.000000\n",
      "step 68500, training accuracy 1.000000\n",
      "step 69000, training accuracy 1.000000\n",
      "step 69500, training accuracy 1.000000\n",
      "step 70000, training accuracy 1.000000\n",
      "step 70500, training accuracy 1.000000\n",
      "step 71000, training accuracy 1.000000\n",
      "step 71500, training accuracy 1.000000\n",
      "step 72000, training accuracy 1.000000\n",
      "step 72500, training accuracy 1.000000\n",
      "step 73000, training accuracy 1.000000\n",
      "step 73500, training accuracy 1.000000\n",
      "step 74000, training accuracy 1.000000\n",
      "step 74500, training accuracy 1.000000\n",
      "test accuracy of the teacher model is 0.658200\n",
      "step 0, training accuracy 0.156250\n",
      "step 500, training accuracy 0.265625\n",
      "step 1000, training accuracy 0.312500\n",
      "step 1500, training accuracy 0.351562\n",
      "step 2000, training accuracy 0.398438\n",
      "step 2500, training accuracy 0.398438\n",
      "step 3000, training accuracy 0.421875\n",
      "step 3500, training accuracy 0.304688\n",
      "step 4000, training accuracy 0.453125\n",
      "step 4500, training accuracy 0.492188\n",
      "step 5000, training accuracy 0.437500\n",
      "step 5500, training accuracy 0.468750\n",
      "step 6000, training accuracy 0.500000\n",
      "step 6500, training accuracy 0.507812\n",
      "step 7000, training accuracy 0.546875\n",
      "step 7500, training accuracy 0.609375\n",
      "step 8000, training accuracy 0.515625\n",
      "step 8500, training accuracy 0.382812\n",
      "step 9000, training accuracy 0.500000\n",
      "step 9500, training accuracy 0.632812\n",
      "step 10000, training accuracy 0.539062\n",
      "step 10500, training accuracy 0.554688\n",
      "step 11000, training accuracy 0.578125\n",
      "step 11500, training accuracy 0.500000\n",
      "step 12000, training accuracy 0.531250\n",
      "step 12500, training accuracy 0.617188\n",
      "step 13000, training accuracy 0.554688\n",
      "step 13500, training accuracy 0.578125\n",
      "step 14000, training accuracy 0.500000\n",
      "step 14500, training accuracy 0.539062\n",
      "step 15000, training accuracy 0.539062\n",
      "step 15500, training accuracy 0.554688\n",
      "step 16000, training accuracy 0.484375\n",
      "step 16500, training accuracy 0.554688\n",
      "step 17000, training accuracy 0.625000\n",
      "step 17500, training accuracy 0.570312\n",
      "step 18000, training accuracy 0.562500\n",
      "step 18500, training accuracy 0.601562\n",
      "step 19000, training accuracy 0.554688\n",
      "step 19500, training accuracy 0.632812\n",
      "step 20000, training accuracy 0.632812\n",
      "step 20500, training accuracy 0.578125\n",
      "step 21000, training accuracy 0.546875\n",
      "step 21500, training accuracy 0.562500\n",
      "step 22000, training accuracy 0.664062\n",
      "step 22500, training accuracy 0.601562\n",
      "step 23000, training accuracy 0.585938\n",
      "step 23500, training accuracy 0.664062\n",
      "step 24000, training accuracy 0.601562\n",
      "step 24500, training accuracy 0.640625\n",
      "step 25000, training accuracy 0.703125\n",
      "step 25500, training accuracy 0.632812\n",
      "step 26000, training accuracy 0.695312\n",
      "step 26500, training accuracy 0.585938\n",
      "step 27000, training accuracy 0.609375\n",
      "step 27500, training accuracy 0.625000\n",
      "step 28000, training accuracy 0.687500\n",
      "step 28500, training accuracy 0.601562\n",
      "step 29000, training accuracy 0.695312\n",
      "step 29500, training accuracy 0.703125\n",
      "step 30000, training accuracy 0.632812\n",
      "step 30500, training accuracy 0.617188\n",
      "step 31000, training accuracy 0.664062\n",
      "step 31500, training accuracy 0.671875\n",
      "step 32000, training accuracy 0.687500\n",
      "step 32500, training accuracy 0.742188\n",
      "step 33000, training accuracy 0.648438\n",
      "step 33500, training accuracy 0.585938\n",
      "step 34000, training accuracy 0.625000\n",
      "step 34500, training accuracy 0.734375\n",
      "step 35000, training accuracy 0.718750\n",
      "step 35500, training accuracy 0.679688\n",
      "step 36000, training accuracy 0.687500\n",
      "step 36500, training accuracy 0.703125\n",
      "step 37000, training accuracy 0.742188\n",
      "step 37500, training accuracy 0.765625\n",
      "step 38000, training accuracy 0.710938\n",
      "step 38500, training accuracy 0.734375\n",
      "step 39000, training accuracy 0.703125\n",
      "step 39500, training accuracy 0.695312\n",
      "step 40000, training accuracy 0.726562\n",
      "step 40500, training accuracy 0.742188\n",
      "step 41000, training accuracy 0.679688\n",
      "step 41500, training accuracy 0.789062\n",
      "step 42000, training accuracy 0.742188\n",
      "step 42500, training accuracy 0.734375\n",
      "step 43000, training accuracy 0.718750\n",
      "step 43500, training accuracy 0.718750\n",
      "step 44000, training accuracy 0.765625\n",
      "step 44500, training accuracy 0.781250\n",
      "step 45000, training accuracy 0.789062\n",
      "step 45500, training accuracy 0.726562\n",
      "step 46000, training accuracy 0.664062\n",
      "step 46500, training accuracy 0.664062\n",
      "step 47000, training accuracy 0.804688\n",
      "step 47500, training accuracy 0.812500\n",
      "step 48000, training accuracy 0.742188\n",
      "step 48500, training accuracy 0.750000\n",
      "step 49000, training accuracy 0.804688\n",
      "step 49500, training accuracy 0.781250\n",
      "step 50000, training accuracy 0.828125\n",
      "step 50500, training accuracy 0.765625\n",
      "step 51000, training accuracy 0.781250\n",
      "step 51500, training accuracy 0.757812\n",
      "step 52000, training accuracy 0.742188\n",
      "step 52500, training accuracy 0.750000\n",
      "step 53000, training accuracy 0.804688\n",
      "step 53500, training accuracy 0.742188\n",
      "step 54000, training accuracy 0.812500\n",
      "step 54500, training accuracy 0.835938\n",
      "step 55000, training accuracy 0.789062\n",
      "step 55500, training accuracy 0.804688\n",
      "step 56000, training accuracy 0.750000\n",
      "step 56500, training accuracy 0.812500\n",
      "step 57000, training accuracy 0.828125\n",
      "step 57500, training accuracy 0.796875\n",
      "step 58000, training accuracy 0.812500\n",
      "step 58500, training accuracy 0.742188\n",
      "step 59000, training accuracy 0.757812\n",
      "step 59500, training accuracy 0.859375\n",
      "step 60000, training accuracy 0.843750\n",
      "step 60500, training accuracy 0.828125\n",
      "step 61000, training accuracy 0.828125\n",
      "step 61500, training accuracy 0.812500\n",
      "step 62000, training accuracy 0.835938\n",
      "step 62500, training accuracy 0.890625\n",
      "step 63000, training accuracy 0.804688\n",
      "step 63500, training accuracy 0.812500\n",
      "step 64000, training accuracy 0.820312\n",
      "step 64500, training accuracy 0.820312\n",
      "step 65000, training accuracy 0.812500\n",
      "step 65500, training accuracy 0.882812\n",
      "step 66000, training accuracy 0.812500\n",
      "step 66500, training accuracy 0.828125\n",
      "step 67000, training accuracy 0.835938\n",
      "step 67500, training accuracy 0.851562\n",
      "step 68000, training accuracy 0.859375\n",
      "step 68500, training accuracy 0.828125\n",
      "step 69000, training accuracy 0.851562\n",
      "step 69500, training accuracy 0.851562\n",
      "step 70000, training accuracy 0.835938\n",
      "step 70500, training accuracy 0.859375\n",
      "step 71000, training accuracy 0.828125\n",
      "step 71500, training accuracy 0.804688\n",
      "step 72000, training accuracy 0.890625\n",
      "step 72500, training accuracy 0.890625\n",
      "step 73000, training accuracy 0.859375\n",
      "step 73500, training accuracy 0.859375\n",
      "step 74000, training accuracy 0.875000\n",
      "step 74500, training accuracy 0.890625\n",
      "step 75000, training accuracy 0.945312\n",
      "step 75500, training accuracy 0.835938\n",
      "step 76000, training accuracy 0.859375\n",
      "step 76500, training accuracy 0.820312\n",
      "step 77000, training accuracy 0.851562\n",
      "step 77500, training accuracy 0.820312\n",
      "step 78000, training accuracy 0.914062\n",
      "step 78500, training accuracy 0.875000\n",
      "step 79000, training accuracy 0.851562\n",
      "step 79500, training accuracy 0.875000\n",
      "step 80000, training accuracy 0.875000\n",
      "step 80500, training accuracy 0.890625\n",
      "step 81000, training accuracy 0.835938\n",
      "step 81500, training accuracy 0.914062\n",
      "step 82000, training accuracy 0.867188\n",
      "step 82500, training accuracy 0.781250\n",
      "step 83000, training accuracy 0.890625\n",
      "step 83500, training accuracy 0.804688\n",
      "step 84000, training accuracy 0.851562\n",
      "step 84500, training accuracy 0.929688\n",
      "step 85000, training accuracy 0.914062\n",
      "step 85500, training accuracy 0.898438\n",
      "step 86000, training accuracy 0.882812\n",
      "step 86500, training accuracy 0.921875\n",
      "step 87000, training accuracy 0.937500\n",
      "step 87500, training accuracy 0.945312\n",
      "step 88000, training accuracy 0.867188\n",
      "step 88500, training accuracy 0.882812\n",
      "step 89000, training accuracy 0.875000\n",
      "step 89500, training accuracy 0.906250\n",
      "step 90000, training accuracy 0.843750\n",
      "step 90500, training accuracy 0.929688\n",
      "step 91000, training accuracy 0.914062\n",
      "step 91500, training accuracy 0.851562\n",
      "step 92000, training accuracy 0.906250\n",
      "step 92500, training accuracy 0.898438\n",
      "step 93000, training accuracy 0.906250\n",
      "step 93500, training accuracy 0.875000\n",
      "step 94000, training accuracy 0.859375\n",
      "step 94500, training accuracy 0.921875\n",
      "step 95000, training accuracy 0.859375\n",
      "step 95500, training accuracy 0.914062\n",
      "step 96000, training accuracy 0.828125\n",
      "step 96500, training accuracy 0.906250\n",
      "step 97000, training accuracy 0.953125\n",
      "step 97500, training accuracy 0.914062\n",
      "step 98000, training accuracy 0.914062\n",
      "step 98500, training accuracy 0.921875\n",
      "step 99000, training accuracy 0.929688\n",
      "step 99500, training accuracy 0.968750\n",
      "step 100000, training accuracy 0.945312\n",
      "step 100500, training accuracy 0.890625\n",
      "step 101000, training accuracy 0.921875\n",
      "step 101500, training accuracy 0.859375\n",
      "step 102000, training accuracy 0.921875\n",
      "step 102500, training accuracy 0.835938\n",
      "step 103000, training accuracy 0.937500\n",
      "step 103500, training accuracy 0.945312\n",
      "step 104000, training accuracy 0.890625\n",
      "step 104500, training accuracy 0.953125\n",
      "step 105000, training accuracy 0.914062\n",
      "step 105500, training accuracy 0.945312\n",
      "step 106000, training accuracy 0.906250\n",
      "step 106500, training accuracy 0.898438\n",
      "step 107000, training accuracy 0.937500\n",
      "step 107500, training accuracy 0.921875\n",
      "step 108000, training accuracy 0.960938\n",
      "step 108500, training accuracy 0.875000\n",
      "step 109000, training accuracy 0.921875\n",
      "step 109500, training accuracy 0.953125\n",
      "step 110000, training accuracy 0.937500\n",
      "step 110500, training accuracy 0.929688\n",
      "step 111000, training accuracy 0.945312\n",
      "step 111500, training accuracy 0.945312\n",
      "step 112000, training accuracy 0.968750\n",
      "step 112500, training accuracy 0.968750\n",
      "step 113000, training accuracy 0.890625\n",
      "step 113500, training accuracy 0.945312\n",
      "step 114000, training accuracy 0.906250\n",
      "step 114500, training accuracy 0.929688\n",
      "step 115000, training accuracy 0.937500\n",
      "step 115500, training accuracy 0.945312\n",
      "step 116000, training accuracy 0.976562\n",
      "step 116500, training accuracy 0.890625\n",
      "step 117000, training accuracy 0.945312\n",
      "step 117500, training accuracy 0.945312\n",
      "step 118000, training accuracy 0.953125\n",
      "step 118500, training accuracy 0.937500\n",
      "step 119000, training accuracy 0.921875\n",
      "step 119500, training accuracy 0.976562\n",
      "step 120000, training accuracy 0.921875\n",
      "step 120500, training accuracy 0.960938\n",
      "step 121000, training accuracy 0.929688\n",
      "step 121500, training accuracy 0.921875\n",
      "step 122000, training accuracy 0.968750\n",
      "step 122500, training accuracy 0.937500\n",
      "step 123000, training accuracy 0.953125\n",
      "step 123500, training accuracy 0.945312\n",
      "step 124000, training accuracy 0.960938\n",
      "step 124500, training accuracy 0.968750\n",
      "step 125000, training accuracy 0.984375\n",
      "step 125500, training accuracy 0.929688\n",
      "step 126000, training accuracy 0.953125\n",
      "step 126500, training accuracy 0.921875\n",
      "step 127000, training accuracy 0.921875\n",
      "step 127500, training accuracy 0.937500\n",
      "step 128000, training accuracy 0.960938\n",
      "step 128500, training accuracy 0.976562\n",
      "step 129000, training accuracy 0.921875\n",
      "step 129500, training accuracy 0.960938\n",
      "step 130000, training accuracy 0.976562\n",
      "step 130500, training accuracy 0.960938\n",
      "step 131000, training accuracy 0.953125\n",
      "step 131500, training accuracy 0.945312\n",
      "step 132000, training accuracy 0.976562\n",
      "step 132500, training accuracy 0.921875\n",
      "step 133000, training accuracy 0.953125\n",
      "step 133500, training accuracy 0.945312\n",
      "step 134000, training accuracy 0.937500\n",
      "step 134500, training accuracy 0.976562\n",
      "step 135000, training accuracy 0.945312\n",
      "step 135500, training accuracy 0.968750\n",
      "step 136000, training accuracy 0.953125\n",
      "step 136500, training accuracy 0.968750\n",
      "step 137000, training accuracy 0.984375\n",
      "step 137500, training accuracy 0.984375\n",
      "step 138000, training accuracy 0.937500\n",
      "step 138500, training accuracy 0.968750\n",
      "step 139000, training accuracy 0.937500\n",
      "step 139500, training accuracy 0.937500\n",
      "step 140000, training accuracy 0.960938\n",
      "step 140500, training accuracy 0.960938\n",
      "step 141000, training accuracy 0.976562\n",
      "step 141500, training accuracy 0.937500\n",
      "step 142000, training accuracy 0.960938\n",
      "step 142500, training accuracy 0.968750\n",
      "step 143000, training accuracy 0.953125\n",
      "step 143500, training accuracy 0.953125\n",
      "step 144000, training accuracy 0.968750\n",
      "step 144500, training accuracy 0.968750\n",
      "step 145000, training accuracy 0.945312\n",
      "step 145500, training accuracy 0.960938\n",
      "step 146000, training accuracy 0.945312\n",
      "step 146500, training accuracy 0.976562\n",
      "step 147000, training accuracy 0.984375\n",
      "step 147500, training accuracy 0.960938\n",
      "step 148000, training accuracy 0.960938\n",
      "step 148500, training accuracy 0.953125\n",
      "step 149000, training accuracy 0.953125\n",
      "step 149500, training accuracy 0.992188\n",
      "step 150000, training accuracy 0.992188\n",
      "step 150500, training accuracy 0.945312\n",
      "step 151000, training accuracy 0.960938\n",
      "step 151500, training accuracy 0.953125\n",
      "step 152000, training accuracy 0.929688\n",
      "step 152500, training accuracy 0.960938\n",
      "step 153000, training accuracy 0.968750\n",
      "step 153500, training accuracy 0.992188\n",
      "step 154000, training accuracy 0.945312\n",
      "step 154500, training accuracy 0.960938\n",
      "step 155000, training accuracy 0.984375\n",
      "step 155500, training accuracy 0.984375\n",
      "step 156000, training accuracy 0.929688\n",
      "step 156500, training accuracy 0.968750\n",
      "step 157000, training accuracy 0.968750\n",
      "step 157500, training accuracy 0.976562\n",
      "step 158000, training accuracy 0.976562\n",
      "step 158500, training accuracy 0.937500\n",
      "step 159000, training accuracy 0.960938\n",
      "step 159500, training accuracy 0.976562\n",
      "step 160000, training accuracy 0.968750\n",
      "step 160500, training accuracy 0.992188\n",
      "step 161000, training accuracy 0.960938\n",
      "step 161500, training accuracy 0.953125\n",
      "step 162000, training accuracy 1.000000\n",
      "step 162500, training accuracy 0.992188\n",
      "step 163000, training accuracy 0.960938\n",
      "step 163500, training accuracy 0.968750\n",
      "step 164000, training accuracy 0.976562\n",
      "step 164500, training accuracy 0.945312\n",
      "step 165000, training accuracy 0.968750\n",
      "step 165500, training accuracy 0.960938\n",
      "step 166000, training accuracy 0.992188\n",
      "step 166500, training accuracy 0.937500\n",
      "step 167000, training accuracy 0.976562\n",
      "step 167500, training accuracy 0.976562\n",
      "step 168000, training accuracy 0.976562\n",
      "step 168500, training accuracy 0.976562\n",
      "step 169000, training accuracy 0.953125\n",
      "step 169500, training accuracy 0.984375\n",
      "step 170000, training accuracy 0.976562\n",
      "step 170500, training accuracy 0.984375\n",
      "step 171000, training accuracy 0.945312\n",
      "step 171500, training accuracy 0.976562\n",
      "step 172000, training accuracy 0.992188\n",
      "step 172500, training accuracy 0.960938\n",
      "step 173000, training accuracy 0.984375\n",
      "step 173500, training accuracy 0.960938\n",
      "step 174000, training accuracy 0.953125\n",
      "step 174500, training accuracy 1.000000\n",
      "step 175000, training accuracy 0.992188\n",
      "step 175500, training accuracy 0.953125\n",
      "step 176000, training accuracy 0.976562\n",
      "step 176500, training accuracy 0.976562\n",
      "step 177000, training accuracy 0.953125\n",
      "step 177500, training accuracy 0.968750\n",
      "step 178000, training accuracy 0.976562\n",
      "step 178500, training accuracy 0.992188\n",
      "step 179000, training accuracy 0.960938\n",
      "step 179500, training accuracy 0.976562\n",
      "step 180000, training accuracy 0.992188\n",
      "step 180500, training accuracy 0.984375\n",
      "step 181000, training accuracy 0.968750\n",
      "step 181500, training accuracy 0.960938\n",
      "step 182000, training accuracy 0.976562\n",
      "step 182500, training accuracy 0.937500\n",
      "step 183000, training accuracy 0.992188\n",
      "step 183500, training accuracy 0.984375\n",
      "step 184000, training accuracy 0.976562\n",
      "step 184500, training accuracy 0.984375\n",
      "step 185000, training accuracy 0.976562\n",
      "step 185500, training accuracy 0.992188\n",
      "step 186000, training accuracy 0.968750\n",
      "step 186500, training accuracy 0.960938\n",
      "step 187000, training accuracy 0.984375\n",
      "step 187500, training accuracy 0.992188\n",
      "step 188000, training accuracy 0.968750\n",
      "step 188500, training accuracy 0.968750\n",
      "step 189000, training accuracy 0.976562\n",
      "step 189500, training accuracy 0.945312\n",
      "step 190000, training accuracy 0.968750\n",
      "step 190500, training accuracy 0.984375\n",
      "step 191000, training accuracy 0.984375\n",
      "step 191500, training accuracy 0.976562\n",
      "step 192000, training accuracy 0.976562\n",
      "step 192500, training accuracy 0.929688\n",
      "step 193000, training accuracy 0.984375\n",
      "step 193500, training accuracy 0.968750\n",
      "step 194000, training accuracy 0.992188\n",
      "step 194500, training accuracy 0.984375\n",
      "step 195000, training accuracy 0.976562\n",
      "step 195500, training accuracy 0.976562\n",
      "step 196000, training accuracy 0.960938\n",
      "step 196500, training accuracy 0.984375\n",
      "step 197000, training accuracy 0.984375\n",
      "step 197500, training accuracy 0.976562\n",
      "step 198000, training accuracy 0.992188\n",
      "step 198500, training accuracy 0.984375\n",
      "step 199000, training accuracy 0.960938\n",
      "step 199500, training accuracy 0.984375\n",
      "step 200000, training accuracy 0.992188\n",
      "step 200500, training accuracy 0.968750\n",
      "step 201000, training accuracy 0.976562\n",
      "step 201500, training accuracy 0.976562\n",
      "step 202000, training accuracy 0.953125\n",
      "step 202500, training accuracy 0.960938\n",
      "step 203000, training accuracy 0.984375\n",
      "step 203500, training accuracy 0.992188\n",
      "step 204000, training accuracy 0.968750\n",
      "step 204500, training accuracy 0.984375\n",
      "step 205000, training accuracy 0.984375\n",
      "step 205500, training accuracy 0.992188\n",
      "step 206000, training accuracy 0.976562\n",
      "step 206500, training accuracy 0.992188\n",
      "step 207000, training accuracy 0.984375\n",
      "step 207500, training accuracy 0.968750\n",
      "step 208000, training accuracy 0.984375\n",
      "step 208500, training accuracy 0.976562\n",
      "step 209000, training accuracy 0.968750\n",
      "step 209500, training accuracy 0.976562\n",
      "step 210000, training accuracy 0.976562\n",
      "step 210500, training accuracy 0.992188\n",
      "step 211000, training accuracy 0.984375\n",
      "step 211500, training accuracy 0.992188\n",
      "step 212000, training accuracy 1.000000\n",
      "step 212500, training accuracy 0.992188\n",
      "step 213000, training accuracy 0.984375\n",
      "step 213500, training accuracy 0.976562\n",
      "step 214000, training accuracy 0.984375\n",
      "step 214500, training accuracy 0.968750\n",
      "step 215000, training accuracy 0.976562\n",
      "step 215500, training accuracy 0.992188\n",
      "step 216000, training accuracy 0.992188\n",
      "step 216500, training accuracy 0.968750\n",
      "step 217000, training accuracy 1.000000\n",
      "step 217500, training accuracy 0.859375\n",
      "step 218000, training accuracy 0.992188\n",
      "step 218500, training accuracy 0.953125\n",
      "step 219000, training accuracy 0.992188\n",
      "step 219500, training accuracy 0.984375\n",
      "step 220000, training accuracy 0.992188\n",
      "step 220500, training accuracy 0.992188\n",
      "step 221000, training accuracy 0.976562\n",
      "step 221500, training accuracy 0.968750\n",
      "step 222000, training accuracy 0.976562\n",
      "step 222500, training accuracy 0.976562\n",
      "step 223000, training accuracy 0.992188\n",
      "step 223500, training accuracy 0.992188\n",
      "step 224000, training accuracy 0.984375\n",
      "step 224500, training accuracy 0.984375\n",
      "step 225000, training accuracy 0.992188\n",
      "step 225500, training accuracy 0.992188\n",
      "step 226000, training accuracy 0.992188\n",
      "step 226500, training accuracy 0.976562\n",
      "step 227000, training accuracy 0.968750\n",
      "step 227500, training accuracy 0.992188\n",
      "step 228000, training accuracy 0.984375\n",
      "step 228500, training accuracy 0.992188\n",
      "step 229000, training accuracy 0.968750\n",
      "step 229500, training accuracy 1.000000\n",
      "step 230000, training accuracy 0.984375\n",
      "step 230500, training accuracy 0.992188\n",
      "step 231000, training accuracy 0.960938\n",
      "step 231500, training accuracy 0.992188\n",
      "step 232000, training accuracy 0.992188\n",
      "step 232500, training accuracy 1.000000\n",
      "step 233000, training accuracy 0.984375\n",
      "step 233500, training accuracy 0.968750\n",
      "step 234000, training accuracy 0.992188\n",
      "step 234500, training accuracy 0.984375\n",
      "step 235000, training accuracy 0.976562\n",
      "step 235500, training accuracy 1.000000\n",
      "step 236000, training accuracy 0.984375\n",
      "step 236500, training accuracy 0.976562\n",
      "step 237000, training accuracy 0.984375\n",
      "step 237500, training accuracy 1.000000\n",
      "step 238000, training accuracy 0.992188\n",
      "step 238500, training accuracy 0.976562\n",
      "step 239000, training accuracy 0.984375\n",
      "step 239500, training accuracy 0.968750\n",
      "step 240000, training accuracy 0.984375\n",
      "step 240500, training accuracy 1.000000\n",
      "step 241000, training accuracy 0.992188\n",
      "step 241500, training accuracy 0.968750\n",
      "step 242000, training accuracy 1.000000\n",
      "step 242500, training accuracy 0.984375\n",
      "step 243000, training accuracy 0.992188\n",
      "step 243500, training accuracy 0.953125\n",
      "step 244000, training accuracy 0.984375\n",
      "step 244500, training accuracy 0.992188\n",
      "step 245000, training accuracy 0.992188\n",
      "step 245500, training accuracy 0.992188\n",
      "step 246000, training accuracy 0.976562\n",
      "step 246500, training accuracy 0.976562\n",
      "step 247000, training accuracy 0.984375\n",
      "step 247500, training accuracy 0.976562\n",
      "step 248000, training accuracy 0.992188\n",
      "step 248500, training accuracy 0.984375\n",
      "step 249000, training accuracy 0.976562\n",
      "step 249500, training accuracy 0.992188\n",
      "step 250000, training accuracy 0.992188\n",
      "step 250500, training accuracy 0.984375\n",
      "step 251000, training accuracy 0.984375\n",
      "step 251500, training accuracy 0.992188\n",
      "step 252000, training accuracy 0.960938\n",
      "step 252500, training accuracy 0.984375\n",
      "step 253000, training accuracy 1.000000\n",
      "step 253500, training accuracy 0.992188\n",
      "step 254000, training accuracy 0.968750\n",
      "step 254500, training accuracy 1.000000\n",
      "step 255000, training accuracy 0.984375\n",
      "step 255500, training accuracy 0.984375\n",
      "step 256000, training accuracy 0.992188\n",
      "step 256500, training accuracy 0.992188\n",
      "step 257000, training accuracy 0.992188\n",
      "step 257500, training accuracy 0.929688\n",
      "step 258000, training accuracy 0.992188\n",
      "step 258500, training accuracy 0.976562\n",
      "step 259000, training accuracy 1.000000\n",
      "step 259500, training accuracy 0.984375\n",
      "step 260000, training accuracy 0.976562\n",
      "step 260500, training accuracy 0.992188\n",
      "step 261000, training accuracy 0.984375\n",
      "step 261500, training accuracy 0.976562\n",
      "step 262000, training accuracy 0.992188\n",
      "step 262500, training accuracy 0.992188\n",
      "step 263000, training accuracy 0.992188\n",
      "step 263500, training accuracy 0.984375\n",
      "step 264000, training accuracy 0.992188\n",
      "step 264500, training accuracy 0.984375\n",
      "step 265000, training accuracy 0.992188\n",
      "step 265500, training accuracy 1.000000\n",
      "step 266000, training accuracy 0.992188\n",
      "step 266500, training accuracy 0.968750\n",
      "step 267000, training accuracy 1.000000\n",
      "step 267500, training accuracy 0.984375\n",
      "step 268000, training accuracy 1.000000\n",
      "step 268500, training accuracy 0.984375\n",
      "step 269000, training accuracy 0.992188\n",
      "step 269500, training accuracy 0.992188\n",
      "step 270000, training accuracy 0.921875\n",
      "step 270500, training accuracy 0.984375\n",
      "step 271000, training accuracy 0.984375\n",
      "step 271500, training accuracy 0.992188\n",
      "step 272000, training accuracy 0.976562\n",
      "step 272500, training accuracy 0.992188\n",
      "step 273000, training accuracy 0.992188\n",
      "step 273500, training accuracy 0.984375\n",
      "step 274000, training accuracy 0.984375\n",
      "step 274500, training accuracy 0.992188\n",
      "step 275000, training accuracy 1.000000\n",
      "step 275500, training accuracy 1.000000\n",
      "step 276000, training accuracy 0.992188\n",
      "step 276500, training accuracy 0.984375\n",
      "step 277000, training accuracy 0.976562\n",
      "step 277500, training accuracy 0.992188\n",
      "step 278000, training accuracy 1.000000\n",
      "step 278500, training accuracy 0.992188\n",
      "step 279000, training accuracy 0.976562\n",
      "step 279500, training accuracy 1.000000\n",
      "step 280000, training accuracy 0.984375\n",
      "step 280500, training accuracy 1.000000\n",
      "step 281000, training accuracy 0.976562\n",
      "step 281500, training accuracy 1.000000\n",
      "step 282000, training accuracy 1.000000\n",
      "step 282500, training accuracy 1.000000\n",
      "step 283000, training accuracy 0.992188\n",
      "step 283500, training accuracy 0.976562\n",
      "step 284000, training accuracy 0.984375\n",
      "step 284500, training accuracy 0.976562\n",
      "step 285000, training accuracy 0.984375\n",
      "step 285500, training accuracy 0.992188\n",
      "step 286000, training accuracy 0.976562\n",
      "step 286500, training accuracy 0.976562\n",
      "step 287000, training accuracy 0.992188\n",
      "step 287500, training accuracy 1.000000\n",
      "step 288000, training accuracy 0.984375\n",
      "step 288500, training accuracy 1.000000\n",
      "step 289000, training accuracy 0.992188\n",
      "step 289500, training accuracy 0.992188\n",
      "step 290000, training accuracy 0.992188\n",
      "step 290500, training accuracy 1.000000\n",
      "step 291000, training accuracy 0.992188\n",
      "step 291500, training accuracy 0.976562\n",
      "step 292000, training accuracy 1.000000\n",
      "step 292500, training accuracy 0.984375\n",
      "step 293000, training accuracy 1.000000\n",
      "step 293500, training accuracy 0.992188\n",
      "step 294000, training accuracy 1.000000\n",
      "step 294500, training accuracy 0.992188\n",
      "step 295000, training accuracy 1.000000\n",
      "step 295500, training accuracy 0.992188\n",
      "step 296000, training accuracy 0.984375\n",
      "step 296500, training accuracy 1.000000\n",
      "step 297000, training accuracy 0.984375\n",
      "step 297500, training accuracy 0.984375\n",
      "step 298000, training accuracy 0.992188\n",
      "step 298500, training accuracy 0.976562\n",
      "step 299000, training accuracy 0.992188\n",
      "step 299500, training accuracy 0.992188\n",
      "test accuracy of the student model is 0.569000\n"
     ]
    }
   ],
   "source": [
    "test_acc_teacher, test_acc_student = runDistillationCIFAR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p6mk8hsu1-fR"
   },
   "outputs": [],
   "source": [
    "def runCIFAR(verbose=True, print_every=1000, iters=100000, scope='teacher', num_hidden_units=1200, keep_prob=0.5, batch_size=128, lr=0.04):\n",
    "    with tf.Graph().as_default():\n",
    "        # load data\n",
    "        (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "        \n",
    "        # one-hot encode the labels\n",
    "        y_train = keras.utils.np_utils.to_categorical(y_train)\n",
    "        y_test = keras.utils.np_utils.to_categorical(y_test)\n",
    "        \n",
    "        # scale the data \n",
    "        x_train = x_train.astype('float32')\n",
    "        x_test = x_test.astype('float32')\n",
    "        x_train /= 255\n",
    "        x_test /= 255\n",
    "        n = x_train.shape[0]\n",
    "\n",
    "        # placeholder\n",
    "        x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "        y_ = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "        dropout_rate = tf.placeholder(tf.float32)\n",
    "        x_flat = tf.contrib.layers.flatten(x)\n",
    "\n",
    "        # network output and divided by t       \n",
    "        y = FCNetwork(x_flat, dropout_rate, num_units=num_hidden_units, scope=scope)\n",
    " \n",
    "        # loss and acc\n",
    "        cross_entropy, accuracy = loss(y, y_)\n",
    "\n",
    "        # use gradient descent to optimize\n",
    "        train_step = tf.train.GradientDescentOptimizer(lr).minimize(cross_entropy)\n",
    "\n",
    "        # sess\n",
    "        sess = tf.InteractiveSession()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # train and test student network\n",
    "        for i in range(iters):\n",
    "            batch = batch_idx(batch_size, n, i)\n",
    "            batch_x = x_train[batch,:]\n",
    "            batch_y = y_train[batch,:]\n",
    "            if i% print_every == 0:\n",
    "                train_acc = accuracy.eval(feed_dict={x:batch_x, y_: batch_y})\n",
    "                print(\"step %d, training accuracy %f\"%(i, train_acc))\n",
    "            train_step.run(feed_dict={x: batch_x, y_: batch_y}) \n",
    "\n",
    "        test_acc = sess.run(accuracy, feed_dict={x:x_test, y_: y_test})\n",
    "        print(\"test accuracy of the student model is %f\"%(test_acc))\n",
    "        \n",
    "        return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1788
    },
    "colab_type": "code",
    "id": "5QFCImG5SvTn",
    "outputId": "ee17f7aa-0f9f-4892-bef7-211d254cfad8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1702: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.078125\n",
      "step 1000, training accuracy 0.414062\n",
      "step 2000, training accuracy 0.406250\n",
      "step 3000, training accuracy 0.476562\n",
      "step 4000, training accuracy 0.546875\n",
      "step 5000, training accuracy 0.515625\n",
      "step 6000, training accuracy 0.601562\n",
      "step 7000, training accuracy 0.664062\n",
      "step 8000, training accuracy 0.539062\n",
      "step 9000, training accuracy 0.585938\n",
      "step 10000, training accuracy 0.648438\n",
      "step 11000, training accuracy 0.671875\n",
      "step 12000, training accuracy 0.742188\n",
      "step 13000, training accuracy 0.773438\n",
      "step 14000, training accuracy 0.710938\n",
      "step 15000, training accuracy 0.726562\n",
      "step 16000, training accuracy 0.703125\n",
      "step 17000, training accuracy 0.812500\n",
      "step 18000, training accuracy 0.789062\n",
      "step 19000, training accuracy 0.890625\n",
      "step 20000, training accuracy 0.875000\n",
      "step 21000, training accuracy 0.867188\n",
      "step 22000, training accuracy 0.976562\n",
      "step 23000, training accuracy 0.906250\n",
      "step 24000, training accuracy 0.960938\n",
      "step 25000, training accuracy 0.843750\n",
      "step 26000, training accuracy 0.937500\n",
      "step 27000, training accuracy 0.914062\n",
      "step 28000, training accuracy 0.968750\n",
      "step 29000, training accuracy 0.953125\n",
      "step 30000, training accuracy 0.960938\n",
      "step 31000, training accuracy 1.000000\n",
      "step 32000, training accuracy 0.976562\n",
      "step 33000, training accuracy 1.000000\n",
      "step 34000, training accuracy 0.921875\n",
      "step 35000, training accuracy 0.953125\n",
      "step 36000, training accuracy 0.992188\n",
      "step 37000, training accuracy 0.992188\n",
      "step 38000, training accuracy 0.992188\n",
      "step 39000, training accuracy 0.992188\n",
      "step 40000, training accuracy 0.992188\n",
      "step 41000, training accuracy 1.000000\n",
      "step 42000, training accuracy 1.000000\n",
      "step 43000, training accuracy 1.000000\n",
      "step 44000, training accuracy 1.000000\n",
      "step 45000, training accuracy 0.851562\n",
      "step 46000, training accuracy 1.000000\n",
      "step 47000, training accuracy 1.000000\n",
      "step 48000, training accuracy 1.000000\n",
      "step 49000, training accuracy 1.000000\n",
      "step 50000, training accuracy 1.000000\n",
      "step 51000, training accuracy 1.000000\n",
      "step 52000, training accuracy 1.000000\n",
      "step 53000, training accuracy 1.000000\n",
      "step 54000, training accuracy 1.000000\n",
      "step 55000, training accuracy 1.000000\n",
      "step 56000, training accuracy 1.000000\n",
      "step 57000, training accuracy 1.000000\n",
      "step 58000, training accuracy 1.000000\n",
      "step 59000, training accuracy 1.000000\n",
      "step 60000, training accuracy 1.000000\n",
      "step 61000, training accuracy 1.000000\n",
      "step 62000, training accuracy 1.000000\n",
      "step 63000, training accuracy 1.000000\n",
      "step 64000, training accuracy 1.000000\n",
      "step 65000, training accuracy 1.000000\n",
      "step 66000, training accuracy 1.000000\n",
      "step 67000, training accuracy 1.000000\n",
      "step 68000, training accuracy 1.000000\n",
      "step 69000, training accuracy 1.000000\n",
      "step 70000, training accuracy 1.000000\n",
      "step 71000, training accuracy 1.000000\n",
      "step 72000, training accuracy 1.000000\n",
      "step 73000, training accuracy 1.000000\n",
      "step 74000, training accuracy 1.000000\n",
      "step 75000, training accuracy 1.000000\n",
      "step 76000, training accuracy 1.000000\n",
      "step 77000, training accuracy 1.000000\n",
      "step 78000, training accuracy 1.000000\n",
      "step 79000, training accuracy 1.000000\n",
      "step 80000, training accuracy 1.000000\n",
      "step 81000, training accuracy 1.000000\n",
      "step 82000, training accuracy 1.000000\n",
      "step 83000, training accuracy 1.000000\n",
      "step 84000, training accuracy 1.000000\n",
      "step 85000, training accuracy 1.000000\n",
      "step 86000, training accuracy 1.000000\n",
      "step 87000, training accuracy 1.000000\n",
      "step 88000, training accuracy 1.000000\n",
      "step 89000, training accuracy 1.000000\n",
      "step 90000, training accuracy 1.000000\n",
      "step 91000, training accuracy 1.000000\n",
      "step 92000, training accuracy 1.000000\n",
      "step 93000, training accuracy 1.000000\n",
      "step 94000, training accuracy 1.000000\n",
      "step 95000, training accuracy 1.000000\n",
      "step 96000, training accuracy 1.000000\n",
      "step 97000, training accuracy 1.000000\n",
      "step 98000, training accuracy 1.000000\n",
      "step 99000, training accuracy 1.000000\n",
      "test accuracy of the student model is 0.550800\n"
     ]
    }
   ],
   "source": [
    "test_student_only_acc = runCIFAR(scope='student', num_hidden_units=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e0C7RWIsiUPv"
   },
   "source": [
    "#### 5.3 Results\n",
    "\n",
    "The results we got are:\n",
    "<ul>\n",
    "    <li> Test accuracy of teacher network: <b>65.82%</b>.</li>\n",
    "    <li> Test accuracy of distilled student network: <b>56.90%</b>.</li>\n",
    "     <li> Test accuracy of undistilled student network: <b>55.08%</b>.</li>\n",
    "</ul>\n",
    "\n",
    "Since we only used very simple CNN, so the accuracy is acceptable on CIFAR-10. In this case, we see the performance of cubersome model is much better than lighter model. In addition, we can see that the improvement that the distilled model achieved over the undistilled model is more evident, compared to MNIST case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Avqixqt2_dVE"
   },
   "source": [
    "### 6 Conclusion and Discussion\n",
    "\n",
    "Distilling the knowledge is a useful technique for various tasks: it can be used to facilitate fast optimization of deep neural network, to improve the performance of a small and shallow network with fewer parameters, and to do transfer learning even when the student model is trained at a different task from the teacher model.\n",
    "\n",
    "For both MNIST and CIFAR-10 datasets in this tutorial, we successfully transferred the knowledge learned from the cumbersome model (\"teacher\") to the smaller model (\"student\"). We showed that the student model with soft targets that learned from the teacher model performed better than the stand-alone student model. Especially, for the MNIST dataset, there was no significant difference between the cumbersome model and the distilled model in terms of test set accuracy. \n",
    "\n",
    "Our key methodology was that before training models, we raised the temperature to increase the entropy of soft targets and obtained a much softer distribution for a student model to be effectively trained on much less data. When training models, we used softmax as an output layer to display probability of each class and make it add up to 1. \n",
    "\n",
    "We found out that there are many advantages of knowledge distillation from the teacher model to the student model: the student model performs better than the stand-alone model, it uses a lighter model, and it can be used when there are not many training data available for the student model. A well-trained teacher can help a student be trained with little amount of data. \n",
    "\n",
    "Since this paper was published in 2015, there have been many efforts to combine knowledge distillation with the existing convolutional neural network. There are recently published papers to discuss how knowledge distillation is used in both classification and regression, such as how to improve low-precision network accuracy and face classification accuracy. We believe that knowledge distillation is an important method in training neural network and will be utilized in various fields in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itjtyqVI_jyf"
   },
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9dG2a4P_ljy"
   },
   "source": [
    "1. Distilling the Knowledge in a Neural Network : https://arxiv.org/pdf/1503.02531.pdf\n",
    "2. The CIFAR-10 dataset: https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "3. Knowledge Distillation: https://medium.com/neural-machines/knowledge-distillation-dc241d7c2322\n",
    "4. Improving neural networks by preventing\n",
    "co-adaptation of feature detectors: https://arxiv.org/pdf/1207.0580.pdf"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial_DumplingHouse.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
